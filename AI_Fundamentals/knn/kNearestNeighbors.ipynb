{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 - Programming Assignment\n",
    "\n",
    "## Directions\n",
    "\n",
    "1. Change the name of this file to be your JHED id as in `jsmith299.ipynb`. Because sure you use your JHED ID (it's made out of your name and not your student id which is just letters and numbers).\n",
    "2. Make sure the notebook you submit is cleanly and fully executed. I do not grade unexecuted notebooks.\n",
    "3. Submit your notebook back in Blackboard where you downloaded this file.\n",
    "\n",
    "*Provide the output **exactly** as requested*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k Nearest Neighbors and Model Evaluation\n",
    "\n",
    "In this programming assignment you will use k Nearest Neighbors (kNN) to build a \"model\" that will estimate the compressive strength of various types of concrete. This assignment has several objectives:\n",
    "\n",
    "1. Implement the kNN algorithm with k=9. Remember...the data + distance function is the model in kNN. In addition to asserts that unit test your code, you should \"test drive\" the model, showing output that a non-technical person could interpret.\n",
    "\n",
    "2. You are going to compare the kNN model above against the baseline model described in the course notes (the mean of the training set's target variable). You should use 10 fold cross validation and Mean Squared Error (MSE):\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum^n_i (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "as the evaluation metric (\"error\"). Refer to the course notes for the format your output should take. Don't forget a discussion of the results.\n",
    "\n",
    "3. use validation curves to tune a *hyperparameter* of the model. \n",
    "In this case, the hyperparameter is *k*, the number of neighbors. Don't forget a discussion of the results.\n",
    "\n",
    "4. evaluate the *generalization error* of the new model.\n",
    "Because you may have just created a new, better model, you need a sense of its generalization error, calculate that. Again, what would you like to see as output here? Refer to the course notes. Don't forget a discussion of the results. Did the new model do better than either model in Q2?\n",
    "\n",
    "5. pick one of the \"Choose Your Own Adventure\" options.\n",
    "\n",
    "Refer to the \"course notes\" for this module for most of this assignment.\n",
    "Anytime you just need test/train split, use fold index 0 for the test set and the remainder as the training set.\n",
    "Discuss any results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "The function `parse_data` loads the data from the specified file and returns a List of Lists. The outer List is the data set and each element (List) is a specific observation. Each value of an observation is for a particular measurement. This is what we mean by \"tidy\" data.\n",
    "\n",
    "The function also returns the *shuffled* data because the data might have been collected in a particular order that *might* bias training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Dict, Tuple, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file_name: str) -> List[List]:\n",
    "    data = []\n",
    "    file = open(file_name, \"r\")\n",
    "    for line in file:\n",
    "        datum = [float(value) for value in line.rstrip().split(\",\")]\n",
    "        data.append(datum)\n",
    "    random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = parse_data(\"concrete_compressive_strength.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[272.8, 181.9, 0.0, 185.7, 0.0, 1012.4, 714.3, 7.0, 19.77]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1,030 observations and each observation has 8 measurements. The data dictionary for this data set tells us the definitions of the individual variables (columns/indices):\n",
    "\n",
    "| Index | Variable | Definition |\n",
    "|-------|----------|------------|\n",
    "| 0     | cement   | kg in a cubic meter mixture |\n",
    "| 1     | slag     | kg in a cubic meter mixture |\n",
    "| 2     | ash      | kg in a cubic meter mixture |\n",
    "| 3     | water    | kg in a cubic meter mixture |\n",
    "| 4     | superplasticizer | kg in a cubic meter mixture |\n",
    "| 5     | coarse aggregate | kg in a cubic meter mixture |\n",
    "| 6     | fine aggregate | kg in a cubic meter mixture |\n",
    "| 7     | age | days |\n",
    "| 8     | concrete compressive strength | MPa |\n",
    "\n",
    "The target (\"y\") variable is a Index 8, concrete compressive strength in (Mega?) [Pascals](https://en.wikipedia.org/wiki/Pascal_(unit))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Splits - n folds\n",
    "\n",
    "With n fold cross validation, we divide our data set into n subgroups called \"folds\" and then use those folds for training and testing. You pick n based on the size of your data set. If you have a small data set--100 observations--and you used n=10, each fold would only have 10 observations. That's probably too small. You want at least 30. At the other extreme, we generally don't use n > 10.\n",
    "\n",
    "With 1,030 observations, n = 10 is fine so we will have 10 folds.\n",
    "`create_folds` will take a list (xs) and split it into `n` equal folds with each fold containing one-tenth of the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(xs: List, n: int) -> List[List[List]]:\n",
    "    k, m = divmod(len(xs), n) #k = numdata/10, m = remainder = 0\n",
    "    # be careful of generators...\n",
    "    return list(xs[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = create_folds(data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always use one of the n folds as a test set (and, sometimes, one of the folds as a *pruning* set but not for kNN), and the remaining folds as a training set.\n",
    "We need a function that'll take our n folds and return the train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(folds: List[List[List]], index: int) -> Tuple[List[List], List[List]]:\n",
    "    training = []\n",
    "    test = []\n",
    "    for i, fold in enumerate(folds):\n",
    "        if i == index:\n",
    "            test = fold\n",
    "        else:\n",
    "            training = training + fold\n",
    "    return training, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the function to give us a train and test datasets where the test set is the fold at index 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = create_train_test(folds, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "927"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers\n",
    "\n",
    "Answer the questions above in the space provided below, adding cells as you need to.\n",
    "Put everything in the helper functions and document them.\n",
    "Document everything (what you're doing and why).\n",
    "If you're not sure what format the output should take, refer to the course notes and what they do for that particular topic/algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: kNN\n",
    "\n",
    "Implement k Nearest Neighbors with k = 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_dist\"></a>\n",
    "### get_dist\n",
    "\n",
    "`get_dist` takes in an example and query, and returns the euclidean distance between these two points. **Used by**: [knn](#knn)\n",
    "\n",
    "* **example**: List: Dataset against which you're measuring distance\n",
    "* **query**: List: Test point we're looking to measure\n",
    "\n",
    "**returns** dist: float Euclidean distance between two points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist(example, query): \n",
    "    assert len(example) == len(query)\n",
    "    sum = 0 #init\n",
    "    \n",
    "    for i in range(len(example)-1): #last col is y \n",
    "        sum += (example[i]-query[i])**2\n",
    "    dist = np.sqrt(sum)\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Tests / Assertions\n",
    "ex = [0, 0, 0]\n",
    "quer = [1, 0, 0]\n",
    "assert get_dist(ex, quer) == 1\n",
    "\n",
    "quer = [1, 1, 1]\n",
    "assert get_dist(ex, quer) == np.sqrt(2)\n",
    "\n",
    "quer = [2, 0, 0]\n",
    "assert get_dist(ex, quer) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"knn_processing\"></a>\n",
    "### knn_processing\n",
    "\n",
    "`knn_processing` takes in the k nearest neighbors, and outputs the processed prediction for y. This instantiation outputs the mean y value of the nearest neighbors. \n",
    "\n",
    "* **nearest**: List[float]: values of the k nearest neighbors\n",
    "\n",
    "**returns** mean_y: average y value of the k nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_processing(nearest: List[float]): \n",
    "    dist_only, y_vals, y_sum, num_y = [], [], 0, 0\n",
    "    for i in range(len(nearest)): \n",
    "        y_sum += nearest[i][1][-1]\n",
    "        num_y += 1\n",
    "\n",
    "    return y_sum / num_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Tests / Assertions\n",
    "test_near = [[0.0, [1.0, 1.0, 1.0, 1.0]]] \n",
    "assert knn_processing(test_near) == 1\n",
    "\n",
    "test_near = [[0.0, [1.0, 1.0, 1.0, 1.0]], [0.0, [1.0, 1.0, 1.0, 2.0]]]\n",
    "assert knn_processing(test_near) == 1.5\n",
    "\n",
    "test_near = [[0.0, [1.0, 1.0, 1.0, 1.0]], [0.0, [1.0, 1.0, 1.0, 2.0]], [0.0, [1.0, 1.0, 1.0, 3.0]]]\n",
    "assert knn_processing(test_near) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"knn\"></a>\n",
    "### knn\n",
    "\n",
    "`knn` implements the k-nearest neighbor algorithm. Takes in a dataset and a query, generates a list of distances to k nearest neighbors, and processes that output. Unit Tests for this block are shown in the test drive section. \n",
    "\n",
    "* **dataset**: List: Main dataset against which we're measuring the query\n",
    "* **query**: List: test point we're measuring / predicting for\n",
    "* **k**: int: number of nearest neighbors to generate\n",
    "\n",
    "**returns** prediction: float y_hat predicted value for query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(dataset, query, k): \n",
    "    distances = []\n",
    "    \n",
    "    for example in dataset: \n",
    "        distances.append([get_dist(example,query), example])\n",
    "    \n",
    "    distances = sorted(distances) #sort on first element\n",
    "    nearest = distances[0:k]\n",
    "    prediction = knn_processing(nearest)\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Test Drive the Model\"></a>\n",
    "### Test Drive the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index #  27\n",
      "Actual [MPa]:  19.93\n",
      "Prediction [MPa]:  26.958888888888893\n",
      "--------\n",
      "Index #  5\n",
      "Actual [MPa]:  13.71\n",
      "Prediction [MPa]:  23.34\n",
      "--------\n",
      "Index #  55\n",
      "Actual [MPa]:  22.95\n",
      "Prediction [MPa]:  35.93333333333333\n"
     ]
    }
   ],
   "source": [
    "index = 27\n",
    "random_query = test[index]\n",
    "pred = knn(test, random_query, 9)\n",
    "print('Index # ', index)\n",
    "print('Actual [MPa]: ', test[index][-1])\n",
    "print('Prediction [MPa]: ', pred)\n",
    "print('--------')\n",
    "\n",
    "index = 5\n",
    "random_query = test[index]\n",
    "pred = knn(test, random_query, 9)\n",
    "print('Index # ', index)\n",
    "print('Actual [MPa]: ', test[index][-1])\n",
    "print('Prediction [MPa]: ', pred)\n",
    "print('--------')\n",
    "\n",
    "index = 55\n",
    "random_query = test[index]\n",
    "pred = knn(test, random_query, 9)\n",
    "print('Index # ', index)\n",
    "print('Actual [MPa]: ', test[index][-1])\n",
    "print('Prediction [MPa]: ', pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Evaluation vs. The Mean\n",
    "\n",
    "Using Mean Squared Error (MSE) as your evaluation metric, evaluate your implement above and the Null model, the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_mean_y\"></a>\n",
    "### get_mean_y\n",
    "\n",
    "`get_mean_y` takes in a dataset (or portion) and averages the y value from all points. The y value is set to the last index in each row. \n",
    "\n",
    "* **dataset**: List: test data used to generate y_mean \n",
    "\n",
    "**returns** y_mean: float value for average y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_y(dataset: List): \n",
    "    sum = 0\n",
    "    \n",
    "    for row in dataset: \n",
    "        sum+=row[-1] #y val\n",
    "    \n",
    "    y_mean = sum / len(dataset)\n",
    "    return y_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Tests / Assertions\n",
    "test_data = [[0, 1], [0, 1], [0,1]]\n",
    "assert get_mean_y(test_data) == 1\n",
    "test_data = [[0, 2], [0, 2], [0,2]]\n",
    "assert get_mean_y(test_data) == 2\n",
    "test_data = [[0, 2], [0, 2], [0,5]]\n",
    "assert get_mean_y(test_data) == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluate_mse\"></a>\n",
    "### evaluate_mse\n",
    "\n",
    "`evaluate_mse` uses Mean Squared Error (MSE) as an evaluation metric, and generates the MSE and mean_y for each fold in the 10-fold cross validation. \n",
    "\n",
    "* **folds**: List: Full dataset, split into (10) folds \n",
    "* **k**: int: number of nearest neighbors used in this analysis\n",
    "\n",
    "**returns** results: Dictionary of MSE and mean y value for each fold: float Euclidean distance between two points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mse(folds: List[List[List]], k: int): \n",
    "    n = len(folds[0])\n",
    "    results = {} #init\n",
    "    for i in range(len(folds)): #iterate over folds\n",
    "        \n",
    "        sum, y_sum = 0, 0 #init for each fold\n",
    "        train, test = create_train_test(folds, i)\n",
    "        \n",
    "        for j in range(len(test)): #iterate over rows\n",
    "            y_cur = test[j][-1] #last element in row\n",
    "            y_hat = knn(train, test[j], k)\n",
    "            y_sum += y_cur\n",
    "            sum += ((y_cur - y_hat)**2)\n",
    "\n",
    "        MSE = (1/len(test)) * sum #changed from train to test in resubmit\n",
    "        y_avg = y_sum / len(test)\n",
    "        results[i] = {'MSE': MSE, 'Fold mean y': y_avg}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_results = evaluate_mse(folds, 9)\n",
    "assert len(mse_results) == 10\n",
    "assert (get_mean_y(folds[0])) == mse_results[0]['Fold mean y']\n",
    "assert (get_mean_y(folds[1])) == mse_results[1]['Fold mean y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluate_mean_mse\"></a>\n",
    "### evaluate_mean_mse\n",
    "\n",
    "`evaluate_mean_mse` uses the same logic as the `evaluate_mse` function, except y_mean is used in place of y_hat predictions. This represents the null model, and also uses 10-fold cross validation.\n",
    "\n",
    "* **folds**: List: Full dataset, split into (10) folds \n",
    "* **k**: int: number of nearest neighbors used in this analysis\n",
    "\n",
    "**returns** results: Dictionary of mean MSE and mean y value for each fold: float Euclidean distance between two points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mean_mse(folds: List[List[List]], k: int): \n",
    "    n = len(folds[0])\n",
    "    results = {} #init\n",
    "    for i in range(len(folds)): #iterate over folds\n",
    "        sum, y_sum = 0, 0 #init for each fold\n",
    "        train, test = create_train_test(folds, i)\n",
    "        y_mean = get_mean_y(train)\n",
    "        \n",
    "        for j in range(len(test)): #iterate over rows\n",
    "            y_cur = test[j][-1] #last element in row\n",
    "            y_sum += y_cur\n",
    "            sum += ((y_cur - y_mean)**2)\n",
    "\n",
    "        mean_MSE = (1/len(test)) * sum #changed from train to test in resubmit\n",
    "        y_avg = y_sum / len(test)\n",
    "        results[i] = {'Mean MSE': mean_MSE, 'Fold mean y': y_avg}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mse_results = evaluate_mean_mse(folds, 9)\n",
    "assert len(mean_mse_results) == 10\n",
    "assert (get_mean_y(folds[0])) == mean_mse_results[0]['Fold mean y']\n",
    "assert (get_mean_y(folds[1])) == mean_mse_results[1]['Fold mean y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"avg_mse\"></a>\n",
    "### avg_mse\n",
    "\n",
    "`avg_mse` takes in the MSE results from the `evaluate` function, and averages the MSE value across all 10 folds. This function returns a float value for average MSE. \n",
    "\n",
    "* **mse_results**: Dict: Dictionary storing 10 folds, and the MSE value for each\n",
    "* **key**: Str: Lookup key for MSE in the dictionary\n",
    "\n",
    "**returns** avg float average MSE across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_mse(mse_results: Dict, key: str): \n",
    "    sum = 0\n",
    "    length = len(mse_results)\n",
    "    for i in range(length): \n",
    "        sum += mse_results[i][key]\n",
    "    avg = sum / length\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Tests / Assertions\n",
    "test_results = mse_results\n",
    "assert len(mse_results) == 10\n",
    "test_dict = {0: {'test': 5}, 1: {'test': 10}}\n",
    "assert avg_mse(test_dict, 'test') == 7.5\n",
    "\n",
    "test_dict = {0: {'test': 2}, 1: {'test': 2}}\n",
    "assert avg_mse(test_dict, 'test') == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE:  86.87077578928441\n",
      "Average Mean-MSE:  279.2620808229211\n"
     ]
    }
   ],
   "source": [
    "print('Average MSE: ', avg_mse(mse_results, 'MSE'))\n",
    "print('Average Mean-MSE: ', avg_mse(mean_mse_results, 'Mean MSE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Results\n",
    "From the cell above, we see the method of using the training set mean results in a MSE of roughly triple that of the knn MSE method. This is essentially comparing knn against a null model, so we would expect the knn method to return a lower error (i.e. lower MSE). This tracks with those expectations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Hyperparameter Tuning\n",
    "\n",
    "Tune the value of k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tune_knn\"></a>\n",
    "### tune_knn\n",
    "\n",
    "`tune_knn` takes in the total folds, and a max k value to test. The function then iterates over each value of k from 1 to *k_max*, and generates the average MSE across all 10 folds at that k value. This function runs a simple loop for k-values, calling previously used functions; no further unit tests / assertions are necessary. \n",
    "\n",
    "* **folds**: List[List[List]]]: Entire dataset, split into 10 folds\n",
    "* **k_max**: int: maximum k value to test\n",
    "\n",
    "**returns** None (print statements for average MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_knn(folds, k_max): \n",
    "\n",
    "    for i in range(1,k_max): \n",
    "        results = evaluate_mse(folds, i)\n",
    "        avg = avg_mse(results, 'MSE')\n",
    "        print('k =', str(i), ', Avg MSE: ', avg)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1 , Avg MSE:  76.14022902912623\n",
      "k = 2 , Avg MSE:  79.20551538834951\n",
      "k = 3 , Avg MSE:  75.90810213592232\n",
      "k = 4 , Avg MSE:  75.5415845145631\n",
      "k = 5 , Avg MSE:  78.87600217864076\n",
      "k = 6 , Avg MSE:  83.58282061218986\n",
      "k = 7 , Avg MSE:  84.08195187834356\n",
      "k = 8 , Avg MSE:  84.28254505916263\n",
      "k = 9 , Avg MSE:  86.87077578928441\n",
      "k = 10 , Avg MSE:  89.5253041893204\n",
      "k = 11 , Avg MSE:  91.6770276731124\n",
      "k = 12 , Avg MSE:  92.8732228229504\n",
      "k = 13 , Avg MSE:  94.27140319411731\n",
      "k = 14 , Avg MSE:  95.16755228452544\n",
      "k = 15 , Avg MSE:  97.02561548392664\n",
      "k = 16 , Avg MSE:  99.2175871609527\n",
      "k = 17 , Avg MSE:  101.33023809554206\n",
      "k = 18 , Avg MSE:  102.6583883629989\n",
      "k = 19 , Avg MSE:  105.07362677002934\n"
     ]
    }
   ],
   "source": [
    "tune_knn(folds, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Results\n",
    "\n",
    "This tuning generally shows that average MSE increases as k increases. This makes some intuitive sense: as k increases to the limit, k is eventually equal to the number of data points, which turns the model into the null model. Lower k-values are ideal in this situation, especially for regression. The farther you expand the zone of influence around a target point (i.e. increase k), the more error the model experiences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Generalization Error\n",
    "\n",
    "Analyze and discuss the generalization error of your model with the value of k from Problem 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed above, MSE generally increases as k increases. There are a few dips in this slope, where MSE appears level or decreasing for a step of k, such as from k=2 to k=3, and from k=8 to k=9. For this evaluation, I'll use k=3, and re-evaluate the same model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_y over training dataset:  35.91021574973034\n",
      "average MSE across normalized results:  75.90810213592232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'MSE': 94.92535577130523, 'Fold mean y': 34.987669902912636},\n",
       " 1: {'MSE': 76.86820258899677, 'Fold mean y': 36.26980582524273},\n",
       " 2: {'MSE': 67.62811154261057, 'Fold mean y': 38.32446601941749},\n",
       " 3: {'MSE': 72.38882524271845, 'Fold mean y': 35.15281553398057},\n",
       " 4: {'MSE': 85.43824390507012, 'Fold mean y': 35.650776699029116},\n",
       " 5: {'MSE': 65.07042384034519, 'Fold mean y': 34.00718446601942},\n",
       " 6: {'MSE': 62.795598381877014, 'Fold mean y': 34.76815533980582},\n",
       " 7: {'MSE': 56.43578673139156, 'Fold mean y': 37.07330097087379},\n",
       " 8: {'MSE': 78.27545512405605, 'Fold mean y': 34.30165048543689},\n",
       " 9: {'MSE': 99.2550182308522, 'Fold mean y': 37.64378640776699}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MSE vs Mean Error\n",
    "gen_results = evaluate_mse(folds, k=3)\n",
    "print('mean_y over training dataset: ', get_mean_y(train))\n",
    "print('average MSE across normalized results: ', avg_mse(gen_results, 'MSE'))\n",
    "gen_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Results\n",
    "In general, the MSE values for k=3 look similar to those of k=9. The average for this evaluation (8.6) is less than the k=9 case (9.9). This seems to be a better choice for k-value moving forward. As an added benefit, this will increase computational efficiency as well, since each test point is only being evaluated against 3 nearest neighbors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5: Choose your own adventure\n",
    "\n",
    "You have three options for the next part:\n",
    "\n",
    "1. You can implement mean normalization (also called \"z-score standardization\") of the *features*; do not normalize the target, y. See if this improves the generalization error of your model (middle).\n",
    "\n",
    "2. You can implement *learning curves* to see if more data would likely improve your model (easiest).\n",
    "\n",
    "3. You can implement *weighted* kNN and use the real valued GA to choose the weights. weighted kNN assigns a weight to each item in the Euclidean distance calculation. For two points, j and k:\n",
    "$$\\sqrt{\\sum w_i (x^k_i - x^j_i)^2}$$\n",
    "\n",
    "You can think of normal Euclidean distance as the case where $w_i = 1$ for all features  (ambitious, but fun...you need to start EARLY because it takes a really long time to run).\n",
    "\n",
    "The easier the adventure the more correct it must be..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question, I will address #1 - normalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"find_max_vals\"></a>\n",
    "### find_max_vals\n",
    "\n",
    "`find_max_vals` takes in a dataset and outputs the maximum value in each column. This will be used to identify the maximum value in each of the data features. \n",
    "\n",
    "* **data**: List: dataset being evaluated\n",
    "\n",
    "**returns** max_vals: List of maximum values in each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_vals(data: List[List]): \n",
    "    max_vals = data[0] #init\n",
    "    for row in data: \n",
    "        for i in range(len(row)): \n",
    "            if row[i] > max_vals[i]: \n",
    "                max_vals[i] = row[i]\n",
    "\n",
    "    return max_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Tests / Assertions\n",
    "test = [[1, 1, 1], [2, 2, 2], [3, 3, 3]]\n",
    "assert find_max_vals(test) == [3, 3, 3]\n",
    "\n",
    "test = [[1, 1, 1], [2, 5, 2], [3, 3, 3]]\n",
    "assert find_max_vals(test) == [3, 5, 3]\n",
    "\n",
    "test = [[1, 1, 7], [2, 5, 2], [3, 3, 3]]\n",
    "assert find_max_vals(test) == [3, 5, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"normalize\"></a>\n",
    "### normalize\n",
    "\n",
    "`normalize` takes in the full dataset, creates a deep copy, determines max values for each feature, then normalizes the dataset (except for the y-column). \n",
    "\n",
    "* **data**: List: full dataset for normalization\n",
    "\n",
    "**returns** norm_data: List of normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data: List[List]): \n",
    "    \n",
    "    norm_data = deepcopy(data)\n",
    "    max_vals = find_max_vals(data)\n",
    "    for i in range(len(norm_data)): #iter rows \n",
    "        for j in range(len(norm_data[0])-1): #iter cols\n",
    "            norm_data[i][j] = float(norm_data[i][j]) / float(max_vals[j])\n",
    "    \n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_y over normalized dataset:  35.81796116504851\n",
      "average MSE across normalized results:  62.347890053937434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'MSE': 66.88365480043147, 'Fold mean y': 34.987669902912636},\n",
       " 1: {'MSE': 57.00971067961165, 'Fold mean y': 36.26980582524273},\n",
       " 2: {'MSE': 58.04900658036675, 'Fold mean y': 38.32446601941749},\n",
       " 3: {'MSE': 49.809030420711956, 'Fold mean y': 35.15281553398057},\n",
       " 4: {'MSE': 71.50795080906147, 'Fold mean y': 35.650776699029116},\n",
       " 5: {'MSE': 63.178303020496216, 'Fold mean y': 34.00718446601942},\n",
       " 6: {'MSE': 58.250506688241636, 'Fold mean y': 34.76815533980582},\n",
       " 7: {'MSE': 43.44902912621359, 'Fold mean y': 37.07330097087379},\n",
       " 8: {'MSE': 78.8639008629989, 'Fold mean y': 34.30165048543689},\n",
       " 9: {'MSE': 76.4778075512406, 'Fold mean y': 37.64378640776699}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = normalize(data)\n",
    "norm_folds = create_folds(norm, 10)\n",
    "norm_results = evaluate_mse(norm_folds, 3)\n",
    "print('mean_y over normalized dataset: ', get_mean_y(norm))\n",
    "print('average MSE across normalized results: ', avg_mse(norm_results, 'MSE'))\n",
    "norm_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Results\n",
    "Normalizing the data seems to improve (decrease) the MSE of the model (from 8.6 to 7.5), reducing the overall generalization error. Normalizing the data reduces the variablility in each of the features, so it would make intuitive sense that the MSE should be reduced. The values in this dataset are on similar orders of magnitude, with the exception of index 4 (superplasticizer), which is a single order of magnitude less. If this dataset were to have values at drastically different orders of magnitude (i.e. 10 and 10000000), I would expect normalization to have a larger impact on reducing the generalization error and improving MSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Resubmittal - Z Score Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_cols\"></a>\n",
    "### get_cols\n",
    "\n",
    "`get_cols` takes in the full dataset and outputs a list of all the column data (except for y-column)\n",
    "\n",
    "* **data**: List: full dataset for normalization\n",
    "\n",
    "**returns** cols: List[List] of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_cols(data: List[List]):\n",
    "    cols = [[] for i in range(len(data[0])-1)] #init 8 empty lists\n",
    "    for row in data: \n",
    "        for i in range(len(data[0])-1): #not incl y\n",
    "            cols[i].append(row[i])\n",
    "\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unit Tests / Assertions\n",
    "test_data = [\n",
    "    [1, 2, 7], \n",
    "    [3, 4, 7]]\n",
    "assert get_cols(test_data) == [[1, 3], [2, 4]]\n",
    "assert len(get_cols(test_data)) == len(test_data[0]) - 1\n",
    "test_data = [\n",
    "    [1, 2], \n",
    "    [3, 4]]\n",
    "assert get_cols(test_data) == [[1, 3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_std_dev\"></a>\n",
    "### get_std_dev\n",
    "\n",
    "`get_std_dev` takes in the full dataset and determines standard deviation for each feature (except for the y-column).\n",
    "\n",
    "* **data**: List: full dataset for normalization\n",
    "\n",
    "**returns** std_dev_vals: List of standard deviation values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std_dev(data: List[List]): \n",
    "    cols = get_cols(data)\n",
    "    std_dev_vals = []\n",
    "    for col in cols: \n",
    "        std_dev_vals.append(np.std(col))\n",
    "\n",
    "    return std_dev_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Tests / Assertions\n",
    "test_data = [\n",
    "    [1, 2, 7], \n",
    "    [3, 4, 7]]\n",
    "assert get_std_dev(test_data) == [1.0, 1.0]\n",
    "test_data = [\n",
    "    [1, 2], \n",
    "    [3, 4]]\n",
    "assert get_std_dev(test_data) == [1.0]\n",
    "test_data = [\n",
    "    [1, 2], \n",
    "    [100, 4]]\n",
    "assert get_std_dev(test_data) == [49.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_col_mean\"></a>\n",
    "### get_col_mean\n",
    "\n",
    "`get_col_mean` takes in the full dataset and determines the mean for each feature (except for the y-column).\n",
    "\n",
    "* **data**: List: full dataset for normalization\n",
    "\n",
    "**returns** mean_vals: List of mean values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_mean(data: List[List]): \n",
    "    cols = get_cols(data)\n",
    "    mean_vals = []\n",
    "    for col in cols: \n",
    "        mean_vals.append(np.mean(col))\n",
    "\n",
    "    return mean_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Tests / Assertions\n",
    "test_data = [\n",
    "    [1, 2, 7], \n",
    "    [3, 4, 7]]\n",
    "assert get_col_mean(test_data) == [2.0, 3.0]\n",
    "test_data = [\n",
    "    [1, 2], \n",
    "    [100, 4]]\n",
    "assert get_col_mean(test_data) == [50.5]\n",
    "assert len(get_col_mean(test_data)) == len(test_data[0]) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"z_standardize\"></a>\n",
    "### z_standardize\n",
    "\n",
    "`z_standardize` takes in the full dataset, creates a deep copy, determines mean values and standard deviations for each feature, then performs z-score standardization (except for the y-column). \n",
    "\n",
    "* **data**: List: full dataset for standardization\n",
    "\n",
    "**returns** norm_data: List of normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_standardize(data: List[List]): \n",
    "\n",
    "    z_data = deepcopy(data)\n",
    "    mean_vals = get_col_mean(z_data)\n",
    "    std_dev_vals = get_std_dev(z_data)\n",
    "    for i in range(len(z_data)): #iter rows \n",
    "        for j in range(len(z_data[0])-1): #iter cols, not y\n",
    "            z_data[i][j] = (float(z_data[i][j]) - float(mean_vals[j])) / float(std_dev_vals[j])\n",
    "    \n",
    "    return z_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    [1, 2, 7], \n",
    "    [3, 4, 7]]\n",
    "assert z_standardize(test_data) == [[-1.0, -1.0, 7], [1.0, 1.0, 7]]\n",
    "test_data = [\n",
    "    [1, 2], \n",
    "    [100, 4]]\n",
    "assert z_standardize(test_data) == [[-1.0, 2], [1.0, 4]]\n",
    "assert len(z_standardize(test_data)) == len(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-Score Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_y over normalized dataset:  35.8789611650485\n",
      "average MSE across normalized results:  74.42918348435813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'MSE': 81.57017249190935, 'Fold mean y': 35.597669902912635},\n",
       " 1: {'MSE': 76.06079417475726, 'Fold mean y': 36.26980582524273},\n",
       " 2: {'MSE': 60.35733786407765, 'Fold mean y': 38.32446601941749},\n",
       " 3: {'MSE': 68.5814316073355, 'Fold mean y': 35.15281553398057},\n",
       " 4: {'MSE': 83.2563334412082, 'Fold mean y': 35.650776699029116},\n",
       " 5: {'MSE': 75.90633786407764, 'Fold mean y': 34.00718446601942},\n",
       " 6: {'MSE': 67.04184412081983, 'Fold mean y': 34.76815533980582},\n",
       " 7: {'MSE': 56.54921488673137, 'Fold mean y': 37.07330097087379},\n",
       " 8: {'MSE': 84.30616634304204, 'Fold mean y': 34.30165048543689},\n",
       " 9: {'MSE': 90.66220204962244, 'Fold mean y': 37.64378640776699}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_data = z_standardize(data)\n",
    "z_folds = create_folds(z_data, 10)\n",
    "z_results = evaluate_mse(z_folds, 3)\n",
    "print('mean_y over normalized dataset: ', get_mean_y(z_data))\n",
    "print('average MSE across normalized results: ', avg_mse(z_results, 'MSE'))\n",
    "z_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Resubmit Results\n",
    "Performing z-score standardization of the data seems to result in roughly the same average MSE of the model (i.e. from 75.1 to 75.3), slightly increasing the overall generalization error. I would expect standardizing the data to reduce the variablility in each of the features, so it would make intuitive sense that the MSE should be reduced. The values in this dataset are on similar orders of magnitude, with the exception of index 4 (superplasticizer), which is a single order of magnitude less. If this dataset were to have values at drastically different orders of magnitude (i.e. 10 and 10000000), I would expect standardization to have a larger impact on reducing the generalization error and improving MSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Did you provide output exactly as requested?\n",
    "2. Did you re-execute the entire notebook? (\"Restart Kernel and Rull All Cells...\")\n",
    "3. If you did not complete the assignment or had difficulty please explain what gave you the most difficulty in the Markdown cell below.\n",
    "4. Did you change the name of the file to `jhed_id.ipynb`?\n",
    "\n",
    "Do not submit any other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resubmit Notes\n",
    "\n",
    "**Feedback from Professor**\n",
    "- Cleanup your code, ie, things like ` if mode == 'default': ` in knn_processing\n",
    "- Your MSE has an error for evaluate_mse: MSE = (1/len(train)) * sum should be len of test\n",
    "- Normalize isn't the same as z-score standardization\n",
    "- You're really close. But your MSE results aren't correct as you don't divide by the correct value - Revise (5)\n",
    "\n",
    "**Resubmission Changes**\n",
    "- Default mode removed in `knn_processing` and all subsequent calls\n",
    "- `evaluate_mse` updated - changed len(train) to len(test)\n",
    "- Section 1.11 created, with all applicable new functions:\n",
    "    - `get_cols`\n",
    "    - `get_std_dev`\n",
    "    - `get_col_mean`\n",
    "    - `z_standardize`\n",
    "- New results for Q5 included (using z-score standardization\n",
    "- New results discussed\n",
    "- Code cleaned up throughout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "117px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
