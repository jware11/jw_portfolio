{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9 - Programming Assignment\n",
    "\n",
    "## Directions\n",
    "\n",
    "1. Change the name of this file to be your JHED id as in `jsmith299.ipynb`. Because sure you use your JHED ID (it's made out of your name and not your student id which is just letters and numbers).\n",
    "2. Make sure the notebook you submit is cleanly and fully executed. I do not grade unexecuted notebooks.\n",
    "3. Submit your notebook back in Blackboard where you downloaded this file.\n",
    "\n",
    "*Provide the output **exactly** as requested*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "For this assignment you will be implementing and evaluating a Naive Bayes Classifier with the same data from last week:\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Mushroom\n",
    "\n",
    "(You should have downloaded it).\n",
    "\n",
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <strong>Important</strong>\n",
    "    <p>\n",
    "        No Pandas. The only acceptable libraries in this class are those contained in the `environment.yml`. No OOP, either. You can used Dicts, NamedTuples, etc. as your abstract data type (ADT) for the the tree and nodes.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "You'll first need to calculate all of the necessary probabilities using a `train` function. A flag will control whether or not you use \"+1 Smoothing\" or not. You'll then need to have a `classify` function that takes your probabilities, a List of instances (possibly a list of 1) and returns a List of Tuples. Each Tuple has the best class in the first position and a dict with a key for every possible class label and the associated *normalized* probability. For example, if we have given the `classify` function a list of 2 observations, we would get the following back:\n",
    "\n",
    "```\n",
    "[(\"e\", {\"e\": 0.98, \"p\": 0.02}), (\"p\", {\"e\": 0.34, \"p\": 0.66})]\n",
    "```\n",
    "\n",
    "when calculating the error rate of your classifier, you should pick the class label with the highest probability; you can write a simple function that takes the Dict and returns that class label.\n",
    "\n",
    "As a reminder, the Naive Bayes Classifier generates the *unnormalized* probabilities from the numerator of Bayes Rule:\n",
    "\n",
    "$$P(C|A) \\propto P(A|C)P(C)$$\n",
    "\n",
    "where C is the class and A are the attributes (data). Since the normalizer of Bayes Rule is the *sum* of all possible numerators and you have to calculate them all, the normalizer is just the sum of the probabilities.\n",
    "\n",
    "You will have the same basic functions as the last module's assignment and some of them can be reused or at least repurposed.\n",
    "\n",
    "`train` takes training_data and returns a Naive Bayes Classifier (NBC) as a data structure. There are many options including namedtuples and just plain old nested dictionaries. **No OOP**.\n",
    "\n",
    "```\n",
    "def train(training_data, smoothing=True):\n",
    "   # returns the Decision Tree.\n",
    "```\n",
    "\n",
    "The `smoothing` value defaults to True. You should handle both cases.\n",
    "\n",
    "`classify` takes a NBC produced from the function above and applies it to labeled data (like the test set) or unlabeled data (like some new data). (This is not the same `classify` as the pseudocode which classifies only one instance at a time; it can call it though).\n",
    "\n",
    "```\n",
    "def classify(nbc, observations, labeled=True):\n",
    "    # returns a list of tuples, the argmax and the raw data as per the pseudocode.\n",
    "```\n",
    "\n",
    "`evaluate` takes a data set with labels (like the training set or test set) and the classification result and calculates the classification error rate:\n",
    "\n",
    "$$error\\_rate=\\frac{errors}{n}$$\n",
    "\n",
    "Do not use anything else as evaluation metric or the submission will be deemed incomplete, ie, an \"F\". (Hint: accuracy rate is not the error rate!).\n",
    "\n",
    "`cross_validate` takes the data and uses 10 fold cross validation (from Module 3!) to `train`, `classify`, and `evaluate`. **Remember to shuffle your data before you create your folds**. I leave the exact signature of `cross_validate` to you but you should write it so that you can use it with *any* `classify` function of the same form (using higher order functions and partial application). If you did so last time, you can reuse it for this assignment.\n",
    "\n",
    "Following Module 3's discussion, `cross_validate` should print out the fold number and the evaluation metric (error rate) for each fold and then the average value (and the variance). What you are looking for here is a consistent evaluation metric cross the folds. You should print the error rates in terms of percents (ie, multiply the error rate by 100 and add \"%\" to the end).\n",
    "\n",
    "To summarize...\n",
    "\n",
    "Apply the Naive Bayes Classifier algorithm to the Mushroom data set using 10 fold cross validation and the error rate as the evaluation metric. You will do this *twice*. Once with smoothing=True and once with smoothing=False. You should follow up with a brief explanation for the similarities or differences in the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports from Previous Module (08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Import Data\"></a>\n",
    "### Import Data\n",
    "\n",
    "This code is copied from the Module 03 programming assignment. `parse_data` line 5 was updated for data type `str`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file_name: str) -> List[List]:\n",
    "    data = []\n",
    "    file = open(file_name, \"r\")\n",
    "    for line in file:\n",
    "        datum = [str(value) for value in line.rstrip().split(\",\")]\n",
    "        data.append(datum)\n",
    "    random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = parse_data(\"agaricus-lepiota.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8124"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Missing Values\n",
    "Per the assignment directions, this code will remove all lines in the data with \"?\" values. Per the dataset description, these values are only expected in attribute #11. \n",
    "\n",
    "<div style=\"background: #4682b4\">\n",
    "Commented out for resubmit, per feedback in initial submit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [row for row in data if \"?\" not in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8124"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Splits - n folds\n",
    "\n",
    "This code is copied from the Module 03 programming assignment. It creates folds from the data, then creates train and test datasets. \n",
    "\n",
    "`create_folds` will take a list (xs) and split it into `n` equal folds with each fold containing one-tenth of the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(xs: List, n: int) -> List[List[List]]:\n",
    "    k, m = divmod(len(xs), n) #k = numdata/10, m = remainder = 0\n",
    "    # be careful of generators...\n",
    "    return list(xs[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = create_folds(data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(folds: List[List[List]], index: int) -> Tuple[List[List], List[List]]:\n",
    "    training = []\n",
    "    test = []\n",
    "    for i, fold in enumerate(folds):\n",
    "        if i == index:\n",
    "            test = fold\n",
    "        else:\n",
    "            training = training + fold\n",
    "    return training, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the function to give us a train and test datasets where the test set is the fold at index 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data = create_train_test(folds, 0)\n",
    "# assert len(training_data) == 5079\n",
    "# assert len(test_data) == 565 #10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_cols\"></a>\n",
    "### get_cols\n",
    "\n",
    "`get_cols` takes in the full dataset and outputs a list of all the column data (including y-column)\n",
    "\n",
    "* **data**: List: full dataset\n",
    "\n",
    "**returns** cols: List[List] of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_cols(data: List[List]):\n",
    "    cols = [[] for i in range(len(data[0]))] #init empty lists\n",
    "    for row in data: \n",
    "        for i in range(len(data[0])): #not incl y\n",
    "            cols[i].append(row[i])\n",
    "\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unit Tests / Assertions\n",
    "test_mat = [\n",
    "    [1, 2, 7], \n",
    "    [3, 4, 7]]\n",
    "assert get_cols(test_mat) == [[1, 3], [2, 4], [7, 7]]\n",
    "assert len(get_cols(test_mat)) == len(test_mat[0])\n",
    "test_mat = [\n",
    "    [1, 2], \n",
    "    [3, 4]]\n",
    "assert get_cols(test_mat) == [[1, 3], [2, 4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_unique\"></a>\n",
    "### get_unique\n",
    "\n",
    "`get_unique` takes in a column and outputs the unique values from that column. \n",
    "\n",
    "* **col**: List: single column of data\n",
    "\n",
    "**returns** unique_vals: List of unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique(col): \n",
    "\n",
    "    unique_vals = [] #init\n",
    "    for val in col: \n",
    "        if val not in unique_vals: \n",
    "            unique_vals.append(val)\n",
    "    \n",
    "    return unique_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Tests / Assertions\n",
    "test1 = [1, 2, 3]\n",
    "assert get_unique(test1) == [1, 2, 3]\n",
    "\n",
    "test2 = [2, 2, 2]\n",
    "assert get_unique(test2) == [2]\n",
    "\n",
    "test3 = [-1, 0, 0]\n",
    "assert get_unique(test3) == [-1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "## Train\n",
    "\n",
    "`train` takes a data set with labels (like the training set or test set), and a boolean indicator using +1 smoothing, and returns a naive bayes classifier dictionary of probabilities. The resulting NBC dictionary has first level keys of attribute numbers, second level keys of attribute values, and third level keys of 'e', 'p', and 'att_total'. The 'att_total' key represents the total probability of that attribute value occuring in the training dataset. A single NBC callout will be used as an assertion to show the format of the output. \n",
    "\n",
    "* **training_data**: List[List[str]]: Dataset for training the NBC\n",
    "* **smoothing**: bool: whether or not to use +1 smoothing\n",
    "\n",
    "**returns** Dict: NBC - naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data, smoothing=True): \n",
    "    smooth = 1 if smoothing else 0\n",
    "    yes, no = 'e', 'p' #init\n",
    "    attributes = list(range(0, len(training_data[0]))) #[1, 2, ... N]\n",
    "    NBC = {} #init\n",
    "    cols = get_cols(training_data) #columns \n",
    "    total_yes, total_no = cols[0].count(yes), cols[0].count(no)\n",
    "    for att in attributes: \n",
    "        vals = get_unique(cols[att])\n",
    "        probs = {} #init\n",
    "        for val in vals: \n",
    "            val_yes = sum(1 for i in range(len(cols[att])) if (cols[att][i] == val and cols[0][i] == yes))\n",
    "            val_no = sum(1 for i in range(len(cols[att])) if (cols[att][i] == val and cols[0][i] == no))\n",
    "            py = (val_yes + smooth) / (total_yes + smooth)\n",
    "            pn = (val_no + smooth) / (total_no + smooth)\n",
    "            pt = (val_yes + val_no) / (total_yes + total_no)\n",
    "            probs[val] = {str(yes): py, str(no): pn, 'att_total': pt}\n",
    "        NBC[att] = probs    \n",
    "    return NBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'e': {'e': 1.0, 'p': 0.3333333333333333, 'att_total': 0.5},\n",
       "  'x': {'e': 0.3333333333333333, 'p': 0.3333333333333333, 'att_total': 0.0},\n",
       "  'y': {'e': 0.3333333333333333, 'p': 0.3333333333333333, 'att_total': 0.0},\n",
       "  't': {'e': 0.3333333333333333, 'p': 0.3333333333333333, 'att_total': 0.0},\n",
       "  'a': {'e': 0.3333333333333333, 'p': 0.3333333333333333, 'att_total': 0.0},\n",
       "  'f': {'e': 0.3333333333333333, 'p': 0.3333333333333333, 'att_total': 0.0},\n",
       "  'c': {'e': 0.3333333333333333, 'p': 0.3333333333333333, 'att_total': 0.0},\n",
       "  'b': {'e': 0.3333333333333333, 'p': 0.3333333333333333, 'att_total': 0.0},\n",
       "  'n': {'e': 0.3333333333333333, 'p': 0.3333333333333333, 'att_total': 0.0},\n",
       "  's': {'e': 0.3333333333333333, 'p': 0.3333333333333333, 'att_total': 0.0},\n",
       "  'w': {'e': 0.3333333333333333, 'p': 0.3333333333333333, 'att_total': 0.0},\n",
       "  'p': {'e': 0.3333333333333333, 'p': 1.0, 'att_total': 0.5},\n",
       "  'o': {'e': 0.3333333333333333, 'p': 0.3333333333333333, 'att_total': 0.0},\n",
       "  'g': {'e': 0.3333333333333333, 'p': 0.3333333333333333, 'att_total': 0.0}}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_train = training_data[0]\n",
    "train(temp_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"probability_of\"></a>\n",
    "### probability_of\n",
    "\n",
    "`probability_of` takes in a single instance (like a row in the test dataset), a label ('e' or 'p'), and the nbc dictionary. This calculates P(value | instance). To use the example from the self-check assignment, this function returns the value of: \n",
    "[ùëÉ(ùë†ùëûùë¢ùëéùëüùëí | ùë¶ùëíùë†) * ùëÉ(ùëôùëéùëüùëîùëí | ùë¶ùëíùë†) * ùëÉ(ùëüùëíùëë | ùë¶ùëíùë†) * ùëÉ(ùë¶ùëíùë†)] √∑ [ùëÉ(ùë†ùëûùë¢ùëéùëüùëí) * ùëÉ(ùëôùëéùëüùëîùëí) * ùëÉ(ùëüùëíùëë)]\n",
    "\n",
    "* **instance**: List[str]: single row of a test dataset (observation)\n",
    "* **label**: str: single label to evaluate, either yes/no, 'e'/'p'\n",
    "* **nbc**: Dict: Naive Bayes Classifier (nbc) dictionary\n",
    "\n",
    "**returns** float: P(value | instance)\n",
    "\n",
    "<div style=\"background: #4682b4\">\n",
    "Denominator calculation was commented out (rather than fully deleted, for consistency), per feedback received after initial submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_of(instance, label, nbc, labeled=True): \n",
    "    numerator_vals, denominator_vals = [], [] #init\n",
    "    start = 1 if labeled else 0 # change start index if unlabeled data (include col 0)\n",
    "    for col in range(start, len(instance)): #columns, skip 0 (label)\n",
    "        val = instance[col] #attribute value\n",
    "        probs = nbc[col][val] #probabilities @ attribute = value\n",
    "        numerator_vals.append(probs[label])\n",
    "        # denominator_vals.append(probs['att_total'])\n",
    "\n",
    "    numerator = np.prod(numerator_vals) * nbc[0][label]['att_total']\n",
    "    # denominator = np.prod(denominator_vals)\n",
    "    \n",
    "    # return (numerator / denominator) \n",
    "    return numerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assertions / unit tests\n",
    "instance = training_data[1]\n",
    "nbc = train(training_data)\n",
    "label = instance[0]\n",
    "opp_label = 'e' if label=='p' else 'p'\n",
    "assert probability_of(instance, label, nbc) > probability_of(instance, opp_label, nbc)\n",
    "\n",
    "instance = training_data[5]\n",
    "label = instance[0]\n",
    "opp_label = 'e' if label=='p' else 'p'\n",
    "assert probability_of(instance, label, nbc) > probability_of(instance, opp_label, nbc)\n",
    "\n",
    "instance = training_data[100]\n",
    "label = instance[0]\n",
    "opp_label = 'e' if label=='p' else 'p'\n",
    "assert probability_of(instance, label, nbc) > probability_of(instance, opp_label, nbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"normalize\"></a>\n",
    "### normalize\n",
    "\n",
    "`normalize` takes in the results from `classify` and `probability_of`, and returns normalized results in the same format (Dict).\n",
    "\n",
    "* **results**: Dict: non-normalized results from the naive bayes classifier\n",
    "\n",
    "**returns** Dict: new_results - normalized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(results: Dict): \n",
    "    new_results = {} #init\n",
    "    results_sum = sum(results.values())\n",
    "    for key in results: \n",
    "        new_results[key] = results[key] / results_sum\n",
    "\n",
    "    return new_results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assertions / unit tests\n",
    "instance = training_data[4]\n",
    "results = {'e': probability_of(instance, 'e', nbc), 'p': probability_of(instance, 'p', nbc)}\n",
    "new_results = normalize(results)\n",
    "# print('original results: ', results)\n",
    "# print('new results: ', new_results)\n",
    "\n",
    "assert sum(results.values()) != 1.0\n",
    "# assert sum(new_results.values()) == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"classify\"></a>\n",
    "### classify\n",
    "\n",
    "`classify` takes in the probabilities (nbc), a list of instances (observations, test data), and returns the best possible class and normalized probability distribution. It is important that the observations are in a List[List] form. \n",
    "\n",
    "* **nbc**: Dict: Naive bayes classifier, dictionary of probabilities\n",
    "* **observations**: List[List[str]]: list of observations, like the training dataset\n",
    "* **labeled**: Bool, whether or not data is labeled in col[0]\n",
    "\n",
    "**returns** tuple: (best, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(nbc: Dict, observations: List[List[str]], labeled=True): \n",
    "    \n",
    "    class_labels = ['e', 'p']\n",
    "    total_results = []\n",
    "    for instance in observations: \n",
    "        results ={}\n",
    "        for label in class_labels: #yes / no\n",
    "            results[label] = probability_of(instance, label, nbc, labeled)\n",
    "        results = normalize(results)\n",
    "        best = max(results, key=results.get) #arg max\n",
    "        total_results.append((best, results))\n",
    "\n",
    "    return total_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assertions / Unit Tests\n",
    "obsrvs = [test_data[5]]\n",
    "nbc = train(training_data)\n",
    "actual_label = obsrvs[0][0]\n",
    "classification = classify(nbc, obsrvs)\n",
    "assert actual_label == classification[0][0]\n",
    "\n",
    "obsrvs = [test_data[12]]\n",
    "actual_label = obsrvs[0][0]\n",
    "classification = classify(nbc, obsrvs)\n",
    "assert actual_label == classification[0][0]\n",
    "\n",
    "obsrvs = [test_data[100]]\n",
    "actual_label = obsrvs[0][0]\n",
    "classification = classify(nbc, obsrvs)\n",
    "assert actual_label == classification[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "`evaluate` takes a data set with labels (like the training set or test set) and the naive bayes classifier, calls the `classify` function, and determines the error rate over that dataset. Since this calls the classify function, and acts as a subset of the `cross_validate` loop, no assertions or unit tests are included here; these can be found under the `cross_validate` function call. \n",
    "\n",
    "$$error\\_rate=\\frac{errors}{n}$$\n",
    "\n",
    "* **data**: List[List[str]]: dataset to evaluate\n",
    "* **nbc**: Dict: naive bayes classifier\n",
    "\n",
    "**returns** error rate, as a percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, nbc, labeled=True): \n",
    "    errors, n = 0, len(data)\n",
    "    eval = classify(nbc, data, labeled)\n",
    "    \n",
    "    for i in range(len(data)): \n",
    "        if data[i][0] != eval[i][0]: #incorrect classification\n",
    "            errors += 1\n",
    "   \n",
    "    error_rate = (errors / n)\n",
    "    \n",
    "    return error_rate*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate\n",
    "\n",
    "`cross_validate` takes in the full dataset and a smoothing operator, creates 10 shuffled folds in the data, then steps through each fold. For each iteration in the for-loop, train and test data is created, a new naive bayes classifier is calculated, and the overall error rate is evaluated and printed. \n",
    "\n",
    "* **data**: List[List[str]]: total dataset\n",
    "* **smoothing**: bool: whether or not to use +1 smoothing\n",
    "\n",
    "**returns** None, print statements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(data, smoothing=True): \n",
    "    errors = []\n",
    "    folds = create_folds(data, 10)\n",
    "    for i in range(len(folds)): \n",
    "        train_data, test_data = create_train_test(folds, i)\n",
    "        nbc = train(train_data, smoothing)\n",
    "        error_rate = evaluate(test_data, nbc, True)\n",
    "        errors.append(error_rate)\n",
    "        print('Fold', i, ' Error: ', error_rate, '%')\n",
    "        \n",
    "    print('Average error rate: ', np.mean(errors), '%') #sum(errors)/len(errors)\n",
    "    print('Standard Deviation: ', np.std(errors), '%')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation WITH smoothing:\n",
      "Fold 0  Error:  4.797047970479705 %\n",
      "Fold 1  Error:  4.182041820418204 %\n",
      "Fold 2  Error:  4.674046740467404 %\n",
      "Fold 3  Error:  5.289052890528905 %\n",
      "Fold 4  Error:  4.433497536945813 %\n",
      "Fold 5  Error:  4.80295566502463 %\n",
      "Fold 6  Error:  4.1871921182266005 %\n",
      "Fold 7  Error:  3.32512315270936 %\n",
      "Fold 8  Error:  5.295566502463054 %\n",
      "Fold 9  Error:  4.926108374384237 %\n",
      "Average error rate:  4.591263277164791 %\n",
      "Standard Deviation:  0.5610548343418379 %\n"
     ]
    }
   ],
   "source": [
    "print('Cross validation WITH smoothing:')\n",
    "cross_validate(data, smoothing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation WITHOUT smoothing:\n",
      "Fold 0  Error:  0.24600246002460024 %\n",
      "Fold 1  Error:  0.36900369003690037 %\n",
      "Fold 2  Error:  0.6150061500615006 %\n",
      "Fold 3  Error:  0.24600246002460024 %\n",
      "Fold 4  Error:  0.3694581280788177 %\n",
      "Fold 5  Error:  0.49261083743842365 %\n",
      "Fold 6  Error:  0.12315270935960591 %\n",
      "Fold 7  Error:  0.12315270935960591 %\n",
      "Fold 8  Error:  0.12315270935960591 %\n",
      "Fold 9  Error:  0.6157635467980296 %\n",
      "Average error rate:  0.332330540054169 %\n",
      "Standard Deviation:  0.18298199108712435 %\n"
     ]
    }
   ],
   "source": [
    "print('Cross validation WITHOUT smoothing:')\n",
    "cross_validate(data, smoothing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion of Results\n",
    "* In this instance, both sets of results are identical, showing that smoothing did not matter. This could be because every feature / attribute has a split between 'e' and 'p' labels (i.e. none are perfectly homogeneous). While I would expect this to change the underlying probabilities, it shouldn't change the normalized distribution, so perhaps this makes sense.\n",
    "* Overall a 38%+ error rate is not good. I'm wondering if my overall classification calculation in `probability_of` is off. There were several different approaches to the self-check, and I was unsure what was actually correct for this calculation. This is the most likely place for an error in my code.\n",
    "* The results are fairly consistent across each fold, which is what we should expect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Did you provide output exactly as requested?\n",
    "2. Did you re-execute the entire notebook? (\"Restart Kernel and Rull All Cells...\")\n",
    "3. If you did not complete the assignment or had difficulty please explain what gave you the most difficulty in the Markdown cell below.\n",
    "4. Did you change the name of the file to `jhed_id.ipynb`?\n",
    "\n",
    "Do not submit any other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit Notes\n",
    "* I'm still not quite sure how to handle labeled = False. I modified the starting column index from 1 to 0 in that case, shown in the `probability_of` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Professor Feedback on Initial Submit: \n",
    "- With Naive Bayes, you can actually keep data points with missing features. One of the cool parts about the algorithm.\n",
    "- Nice work trying to keep your code generic, ie, things like `yes, no = 'e', 'p' #init`\n",
    "- Next step would be to drive that info from the dataset.\n",
    "- Like you said in your discussion, your error is pretty high, which makes me thing that there's something funky going on.\n",
    "- I'm not sure why you're dividing by the denominator. Based on your code and your print statements, you have the probability of a feature for a given class.\n",
    "- [P(square) * P(large) * P(red)] - isn't needed.\n",
    "- Additionally, you need to multiple times the probability of the class, irrespective of the data point values\n",
    "- (5/10) - Revise. I think you're close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes Made in Resubmit: \n",
    "- Section 1.3.2 `remove missing values` was commented out, as were the assert statements in 1.3.3. I left the blocks in rather than removing entirely to preserve initial submit structure. \n",
    "- `probability_of` - denominator calculation removed entirely\n",
    "- `evaluate` was passing a single column value (like 'e' or 'p') into the `classify` function, which was causing all sorts of issues. The `evaluate` function was rewritten to pass the entire dataset in to the `classify` function (as was initially intended)\n",
    "- Incorrect classification calculation within `evaluate` was updated and the encompassing loop was entirely restructured to accurately interpret each prediction.\n",
    "- All changes resulted in a much better error rate of ~4.5% with smoothing and 0.32% without smoothing!! This is much more in line with what should be expected. This means that the evaluation *without* smoothing far outperformed the smoothing version by essentially an order of magnitude. This is likely due to the fact we were using +1 smoothing, rather than a smaller or more adaptive measure. In the future we could run multiple validation steps to determine the 'best' smoothing value.- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
