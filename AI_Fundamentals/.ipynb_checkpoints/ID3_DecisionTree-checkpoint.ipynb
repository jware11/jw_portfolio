{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8 - Programming Assignment\n",
    "\n",
    "## Directions\n",
    "\n",
    "1. Change the name of this file to be your JHED id as in `jsmith299.ipynb`. Because sure you use your JHED ID (it's made out of your name and not your student id which is just letters and numbers).\n",
    "2. Make sure the notebook you submit is cleanly and fully executed. I do not grade unexecuted notebooks.\n",
    "3. Submit your notebook back in Blackboard where you downloaded this file.\n",
    "\n",
    "*Provide the output **exactly** as requested*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "For this assignment you will be implementing and evaluating a Decision Tree using the ID3 Algorithm (**no** pruning or normalized information gain). Use the provided pseudocode. The data is located at (copy link):\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Mushroom\n",
    "\n",
    "**Just in case** the UCI repository is down, which happens from time to time, I have included the data and name files on Blackboard.\n",
    "\n",
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <strong>Important</strong>\n",
    "    <p>\n",
    "        No Pandas. The only acceptable libraries in this class are those contained in the `environment.yml`. No OOP, either. You can used Dicts, NamedTuples, etc. as your abstract data type (ADT) for the the tree and nodes.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "One of the things we did not talk about in the lectures was how to deal with missing values. There are two aspects of the problem here. What do we do with missing values in the training data? What do we do with missing values when doing classifcation?\n",
    "\n",
    "There are a lot of different ways that we can handle this.\n",
    "A common algorithm is to use something like kNN to impute the missing values.\n",
    "We can use conditional probability as well.\n",
    "There are also clever modifications to the Decision Tree algorithm itself that one can make.\n",
    "\n",
    "We're going to do something simpler, given the size of the data set: remove the observations with missing values (\"?\").\n",
    "\n",
    "You must implement the following functions:\n",
    "\n",
    "`train` takes training_data and returns the Decision Tree as a data structure.\n",
    "\n",
    "```\n",
    "def train(training_data):\n",
    "   # returns the Decision Tree.\n",
    "```\n",
    "\n",
    "`classify` takes a tree produced from the function above and applies it to labeled data (like the test set) or unlabeled data (like some new data).\n",
    "\n",
    "```\n",
    "def classify(tree, observations, labeled=True):\n",
    "    # returns a list of classifications\n",
    "```\n",
    "\n",
    "`evaluate` takes a data set with labels (like the training set or test set) and the classification result and calculates the classification error rate:\n",
    "\n",
    "$$error\\_rate=\\frac{errors}{n}$$\n",
    "\n",
    "Do not use anything else as evaluation metric or the submission will be deemed incomplete, ie, an \"F\". (Hint: accuracy rate is not the error rate!).\n",
    "\n",
    "`cross_validate` takes the data and uses 10 fold cross validation (from Module 3!) to `train`, `classify`, and `evaluate`. **Remember to shuffle your data before you create your folds**. I leave the exact signature of `cross_validate` to you but you should write it so that you can use it with *any* `classify` function of the same form (using higher order functions and partial application).\n",
    "\n",
    "Following Module 3's assignment, `cross_validate` should print out a table in exactly the same format. What you are looking for here is a consistent evaluation metric cross the folds. Print the error rate to 4 decimal places. **Do not convert to a percentage.**\n",
    "\n",
    "```\n",
    "def pretty_print_tree(tree):\n",
    "    # pretty prints the tree\n",
    "```\n",
    "\n",
    "This should be a text representation of a decision tree trained on the entire data set (no train/test).\n",
    "\n",
    "To summarize...\n",
    "\n",
    "Apply the Decision Tree algorithm to the Mushroom data set using 10 fold cross validation and the error rate as the evaluation metric. When you are done, apply the Decision Tree algorithm to the entire data set and print out the resulting tree.\n",
    "\n",
    "**Note** Because this assignment has a natural recursive implementation, you should consider using `deepcopy` at the appropriate places.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Import Data\"></a>\n",
    "## Import Data\n",
    "\n",
    "This code is copied from the Module 03 programming assignment. `parse_data` line 5 was updated for data type `str`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file_name: str) -> List[List]:\n",
    "    data = []\n",
    "    file = open(file_name, \"r\")\n",
    "    for line in file:\n",
    "        datum = [str(value) for value in line.rstrip().split(\",\")]\n",
    "        data.append(datum)\n",
    "    random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = parse_data(\"agaricus-lepiota.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8124"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Missing Values\n",
    "Per the assignment directions, this code will remove all lines in the data with \"?\" values. Per the dataset description, these values are only expected in attribute #11. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [row for row in data if \"?\" not in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5644"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Splits - n folds\n",
    "\n",
    "This code is copied from the Module 03 programming assignment. It creates folds from the data, then creates train and test datasets. \n",
    "\n",
    "`create_folds` will take a list (xs) and split it into `n` equal folds with each fold containing one-tenth of the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(xs: List, n: int) -> List[List[List]]:\n",
    "    k, m = divmod(len(xs), n) #k = numdata/10, m = remainder = 0\n",
    "    # be careful of generators...\n",
    "    return list(xs[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = create_folds(data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(folds: List[List[List]], index: int) -> Tuple[List[List], List[List]]:\n",
    "    training = []\n",
    "    test = []\n",
    "    for i, fold in enumerate(folds):\n",
    "        if i == index:\n",
    "            test = fold\n",
    "        else:\n",
    "            training = training + fold\n",
    "    return training, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the function to give us a train and test datasets where the test set is the fold at index 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data = create_train_test(folds, 0)\n",
    "assert len(training_data) == 5079\n",
    "assert len(test_data) == 565 #10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pick_best_attribute` subfunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_cols\"></a>\n",
    "### get_cols\n",
    "\n",
    "`get_cols` takes in the full dataset and outputs a list of all the column data (including y-column)\n",
    "\n",
    "* **data**: List: full dataset\n",
    "\n",
    "**returns** cols: List[List] of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_cols(data: List[List]):\n",
    "    cols = [[] for i in range(len(data[0]))] #init empty lists\n",
    "    for row in data: \n",
    "        for i in range(len(data[0])): #not incl y\n",
    "            cols[i].append(row[i])\n",
    "\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unit Tests / Assertions\n",
    "test_mat = [\n",
    "    [1, 2, 7], \n",
    "    [3, 4, 7]]\n",
    "assert get_cols(test_mat) == [[1, 3], [2, 4], [7, 7]]\n",
    "assert len(get_cols(test_mat)) == len(test_mat[0])\n",
    "test_mat = [\n",
    "    [1, 2], \n",
    "    [3, 4]]\n",
    "assert get_cols(test_mat) == [[1, 3], [2, 4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_unique\"></a>\n",
    "### get_unique\n",
    "\n",
    "`get_unique` takes in a column and outputs the unique values from that column. \n",
    "\n",
    "* **col**: List: single column of data\n",
    "\n",
    "**returns** unique_vals: List of unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique(col): \n",
    "\n",
    "    unique_vals = [] #init\n",
    "    for val in col: \n",
    "        if val not in unique_vals: \n",
    "            unique_vals.append(val)\n",
    "    \n",
    "    return unique_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Tests / Assertions\n",
    "test1 = [1, 2, 3]\n",
    "assert get_unique(test1) == [1, 2, 3]\n",
    "\n",
    "test2 = [2, 2, 2]\n",
    "assert get_unique(test2) == [2]\n",
    "\n",
    "test3 = [-1, 0, 0]\n",
    "assert get_unique(test3) == [-1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_domain\"></a>\n",
    "### get_domain\n",
    "\n",
    "`get_domain` takes in the dataset and a column index (`best_attr`), cols the `get_cols` function and `get_unique` function, and returns the domain of a given attribute. This is the same style output of `get_unique`, though this function can start from the full dataset. This is a cleaner implementation, but was created after `get_unique`, so both remain. \n",
    "\n",
    "* **data**: List: full dataset\n",
    "* **best_attr**: List: single column index\n",
    "\n",
    "**returns** unique_vals: List of unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain(data, best_attr): \n",
    "    cols = get_cols(data)\n",
    "    test_col = cols[best_attr]\n",
    "    return get_unique(test_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Tests / Assertions\n",
    "test1 = [[1, 2, -1], \n",
    "         [2, 2, 0], \n",
    "         [3, 2, 0]]\n",
    "assert get_domain(test1, 2) == [-1, 0]\n",
    "assert get_domain(test1, 1) == [2]\n",
    "\n",
    "test2 = [[1, 2, 3]]\n",
    "assert get_domain(test2, 0) == [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"calc_e\"></a>\n",
    "### calc_e\n",
    "\n",
    "`calc_e` takes in number of yes & no items, and total items, and performs the basic entropy calculation. Given this is a basic math function, no unit tests / assertions are performed. \n",
    "\n",
    "* **num_yes**: int: number of observations resulting in positive / yes / 'e' edible label\n",
    "* **num_no**: int: number of observations resulting in negative / no / 'p' poisonous label\n",
    "* **total**: int: total observations\n",
    "\n",
    "**returns** E: float, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_e(num_yes, num_no, total): \n",
    "    # with np.errstate(divide='ignore', invalid='ignore'): #handle divide by 0\n",
    "    E_yes = num_yes / total\n",
    "    E_no = num_no / total\n",
    "    E = 0.0\n",
    "    \n",
    "    if E_yes != 0: \n",
    "        E -= (E_yes * np.log2(E_yes))\n",
    "    if E_no != 0: \n",
    "        E -= (E_no * np.log2(E_no))\n",
    "    \n",
    "    return E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_initial_entropy\"></a>\n",
    "### get_initial_entropy\n",
    "\n",
    "`get_initial_entropy` takes in the label column `y`, and calculates the initial entropy for a given dataset. This is different than the `get_entropy` function because it only considers the `y` column and no attributes. 'e' and 'p' labels are set in these functions; in the future they could be passed in as function inputs. This is a subfunction of `pick_best_attribute`; unit tests and assertions will be performed there. \n",
    "\n",
    "* **y**: List: label column of data\n",
    "\n",
    "**returns** Ew: float, weighted entropy based on number of yes/no items in total set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_entropy(y): \n",
    "    yes, no = 'e', 'p'\n",
    "    Ew = []\n",
    "    \n",
    "    total = len(y) #total number\n",
    "    num_yes = sum(1 for i in range(len(y)) if (y[i] == yes))\n",
    "    num_no = sum(1 for i in range(len(y)) if (y[i] == no))\n",
    "    # E = - (num_yes / total)*np.log2(num_yes / total) - (num_no/total)*np.log2(num_no/total)\n",
    "    E = calc_e(num_yes, num_no, total)\n",
    "\n",
    "    weight = (num_yes+num_no) / total\n",
    "    Ew.append(E*weight)\n",
    "\n",
    "    return sum(Ew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_entropy\"></a>\n",
    "### get_entropy\n",
    "\n",
    "`get_entropy` takes in an attribute column `att_col` and the label column `y`, and calculates the entropy for that attribute. 'e' and 'p' labels are set in these functions; in the future they could be passed in as function inputs. This is a subfunction of `pick_best_attribute`; unit tests and assertions will be performed there. \n",
    "\n",
    "* **att_col**: List: column of data for a given attribute\n",
    "* **y**: List: label column of data\n",
    "\n",
    "**returns** Ew: float, weighted entropy based on number of yes/no items in total set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(att_col, y): #weighted entropy\n",
    "\n",
    "    yes, no = 'e', 'p'\n",
    "    unique_vals = get_unique(att_col)\n",
    "    len_col = len(att_col)\n",
    "    Ew = [] #init\n",
    "\n",
    "    for val in unique_vals: \n",
    "        total = att_col.count(val) #total number\n",
    "        num_yes = sum(1 for i in range(len(att_col)) if (att_col[i] == val and y[i] == yes))\n",
    "        num_no = sum(1 for i in range(len(att_col)) if (att_col[i] == val and y[i] == no))\n",
    "\n",
    "        E = calc_e(num_yes, num_no, total)\n",
    "        \n",
    "        weight = (num_yes+num_no) / len_col\n",
    "        Ew.append(E*weight)\n",
    "\n",
    "    return sum(Ew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pick_best_attribute\"></a>\n",
    "### pick_best_attribute\n",
    "\n",
    "`pick_best_attribute` takes in the dataset and a list of attribute indices, and outputs the current best attribute. This uses several helper functions defined above. \n",
    "\n",
    "* **data**: List[list]: full dataset\n",
    "* **attributes**: List[int]: attribute indices\n",
    "\n",
    "**returns** best_att: int, best attribute index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_best_attribute(data, attributes): \n",
    "\n",
    "    cols = get_cols(data) #all cols\n",
    "    y = cols[0]\n",
    "    entropy_0 = get_initial_entropy(y) #starting\n",
    "    best_att, best_ig = -1, 0 #init\n",
    "\n",
    "    for att in attributes: \n",
    "        info = get_entropy(cols[att], y)\n",
    "        ig = entropy_0 - info\n",
    "        if ig >= best_ig: \n",
    "            best_att, best_ig = att, ig\n",
    "        \n",
    "    return best_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unit tests / assertions\n",
    "test_mat = [\n",
    "    ['e', 2, 7], \n",
    "    ['e', 4, 7], \n",
    "    ['p', 6, 6]]\n",
    "assert pick_best_attribute(test_mat, [1, 2]) == 2\n",
    "\n",
    "test_mat = [\n",
    "    ['e', 2, 1, 5, 3], \n",
    "    ['e', 5, 7, 2, 3], \n",
    "    ['p', 2, 4, 6, 6], \n",
    "    ['e', 8, 9, 2, 3]]\n",
    "assert pick_best_attribute(test_mat, [1, 2, 3, 4]) == 4\n",
    "assert pick_best_attribute(test_mat, [1, 2, 3]) == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"majority_label\"></a>\n",
    "### majority_label\n",
    "\n",
    "`majority_label` takes in the dataset, assumes the label column is at col[0], and returns the majority label from these values. \n",
    "\n",
    "* **data**: List[list]: full dataset\n",
    "\n",
    "**returns** maj_label: majority label from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_label(data): \n",
    "    \n",
    "    vals = get_domain(data, 0)\n",
    "    maj_count, maj_label = 0, vals[0] #y[0]\n",
    "    \n",
    "    for val in vals: \n",
    "        sub_count = sum(1 for i in vals if i == val)\n",
    "        if sub_count > maj_count: \n",
    "            maj_count = sub_count\n",
    "            maj_label = val\n",
    "\n",
    "    return maj_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mat = [\n",
    "    ['e', 2, 1, 5, 3], \n",
    "    ['e', 5, 7, 2, 3], \n",
    "    ['p', 2, 4, 6, 6], \n",
    "    ['e', 8, 9, 2, 3]]\n",
    "assert majority_label(test_mat) == 'e'\n",
    "\n",
    "test_mat = [\n",
    "    ['p', 2, 1, 5, 3], \n",
    "    ['p', 5, 7, 2, 3], \n",
    "    ['p', 2, 4, 6, 6], \n",
    "    ['e', 8, 9, 2, 3]]\n",
    "assert majority_label(test_mat) == 'p'\n",
    "\n",
    "test_mat = [\n",
    "    ['e', 2, 1, 5, 3], \n",
    "    ['e', 5, 7, 2, 3], \n",
    "    ['p', 2, 4, 6, 6], \n",
    "    ['p', 8, 9, 2, 3]]\n",
    "assert majority_label(test_mat) == 'e' #first to reach tied majority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"is_homogeneous\"></a>\n",
    "### is_homogeneous\n",
    "\n",
    "`is_homogeneous` takes in the dataset, assumes the label column is at col[0], and determines if the data all has the same label.\n",
    "\n",
    "* **data**: List[list]: full dataset\n",
    "\n",
    "**returns** homogeneous: bool, True/False on whether cols[0] is all one value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_homogeneous(data): \n",
    "    cols = get_cols(data)\n",
    "    y = cols[0]\n",
    "    val = y[0]\n",
    "    homogeneous = True\n",
    "    for i in y: \n",
    "        if i != val: \n",
    "            homogeneous = False\n",
    "\n",
    "    return homogeneous    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unit Tests / Assertions\n",
    "test_mat = [\n",
    "    ['e', 2, 1, 5, 3], \n",
    "    ['e', 5, 7, 2, 3], \n",
    "    ['p', 2, 4, 6, 6], \n",
    "    ['p', 8, 9, 2, 3]]\n",
    "assert is_homogeneous(test_mat) == False\n",
    "\n",
    "test_mat = [\n",
    "    ['e', 2, 1, 5, 3], \n",
    "    ['e', 5, 7, 2, 3], \n",
    "    ['e', 2, 4, 6, 6], \n",
    "    ['e', 8, 9, 2, 3]]\n",
    "assert is_homogeneous(test_mat) == True\n",
    "\n",
    "test_mat = [\n",
    "    ['e', 2, 1, 5, 3], \n",
    "    ['e', 5, 7, 2, 3], \n",
    "    ['e', 2, 4, 6, 6], \n",
    "    ['p', 8, 9, 2, 3]]\n",
    "assert is_homogeneous(test_mat) == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id3\"></a>\n",
    "### id3\n",
    "\n",
    "`id3` is the main recursive function to generate a decision tree. This takes in the full dataset, a set of attribute indices, and the default (majority) label for the dataset. This recursively builds the decision tree by forming subsets of data and calling itself. Unit tests and assertions are shown in the following sections; `train` is a main function that calls on ID3. \n",
    "\n",
    "* **data**: List[list]: full dataset\n",
    "* **attributes**: List[int]: attribute indices\n",
    "* **default**: str: majority / default attribute value\n",
    "\n",
    "**returns** Dict: best attributes and children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3(data, attributes, default): \n",
    "    if not data: \n",
    "        return default #if not data else \n",
    "    if is_homogeneous(data): \n",
    "        return data[0][0] #class label\n",
    "    if not attributes: \n",
    "        return majority_label(data)\n",
    "    best_attr = pick_best_attribute(data, attributes)\n",
    "    nodes = {}\n",
    "    default_label = majority_label(data)\n",
    "    domain = get_domain(data, best_attr)\n",
    "    for value in domain: \n",
    "        subset = deepcopy(data)\n",
    "        subset = [row for row in subset if row[best_attr]==value]\n",
    "        new_attr = deepcopy(attributes)\n",
    "        new_attr.remove(best_attr)       \n",
    "        child = id3(subset, new_attr, default_label)\n",
    "        nodes[value] = child\n",
    "    return {best_attr: nodes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "## train\n",
    "\n",
    "`train` takes in the training dataset, and outputs the decision tree. \n",
    "\n",
    "* **training_data**: List[List[str]]]: training portion of the dataset, 9 folds\n",
    "\n",
    "**returns** Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data):\n",
    "    \n",
    "    maj = majority_label(training_data)\n",
    "    attributes = list(range(1, len(training_data[0])))\n",
    "    tree = id3(training_data, attributes, maj)\n",
    "\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5: {'n': {20: {'k': 'e', 'n': 'e', 'w': {21: {'y': 'e', 'v': 'e', 'c': 'p'}}, 'r': 'p'}}, 'f': 'p', 'p': 'p', 'a': 'e', 'l': 'e', 'c': 'p', 'm': 'p'}}\n"
     ]
    }
   ],
   "source": [
    "#test output: \n",
    "ans = train(training_data)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"classify\"></a>\n",
    "## classify\n",
    "\n",
    "`classify` takes a tree produced from the function above and applies it to labeled data (like the test set) or unlabeled data (like some new data). `observations` is a single row of attribute data. \n",
    "\n",
    "* **tree**: Dict: decision tree\n",
    "* **observations**: List[str]: single row of attribute data\n",
    "* **labeled**: Bool: whether or not first column is labels for data\n",
    "\n",
    "**eval** str: prediction for label of observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, observations, labeled=True):\n",
    "    \n",
    "    for key in tree.keys():\n",
    "        if labeled == False: \n",
    "            key = key - 1\n",
    "        val = observations[key]\n",
    "        next_val = tree[key][val]\n",
    "        if isinstance(next_val, str): \n",
    "            eval = deepcopy(next_val)\n",
    "        else: \n",
    "            eval = classify(next_val, observations, labeled=True)\n",
    "\n",
    "    return eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_row = test_data[4]\n",
    "ans = train(training_data) #tree \n",
    "eval = classify(ans, test_row) #prediction\n",
    "assert eval == test_row[0]\n",
    "\n",
    "test_row = test_data[0]\n",
    "assert classify(ans, test_row, True) == test_row[0]\n",
    "\n",
    "test_row = test_data[1]\n",
    "assert classify(ans, test_row, True) == test_row[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluate\"></a>\n",
    "## evaluate\n",
    "\n",
    "`evaluate` takes a data set with labels (like the training set or test set) and the classification result and calculates the classification error rate:\n",
    "\n",
    "$$error\\_rate=\\frac{errors}{n}$$\n",
    "\n",
    "* **data**: List[List[str]]: Full dataset\n",
    "\n",
    "*returns print statements*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data): \n",
    "    for i in range(len(folds)): \n",
    "        train_data, test_data = create_train_test(folds, i)\n",
    "        tree = train(test_data)\n",
    "        # evals = []\n",
    "        errors, n = 0, len(test_data)\n",
    "        for row in test_data: \n",
    "            # evals.append(classify(tree, row, True))\n",
    "            eval = classify(tree, row, True)\n",
    "            if eval != row[0]: #incorrect classification\n",
    "                errors += 1\n",
    "        error_rate = (errors / n)\n",
    "        print('Fold ', i, ' Error: ', error_rate*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0  Error:  0.0 %\n",
      "Fold  1  Error:  0.0 %\n",
      "Fold  2  Error:  0.0 %\n",
      "Fold  3  Error:  0.0 %\n",
      "Fold  4  Error:  0.0 %\n",
      "Fold  5  Error:  0.0 %\n",
      "Fold  6  Error:  0.0 %\n",
      "Fold  7  Error:  0.0 %\n",
      "Fold  8  Error:  0.0 %\n",
      "Fold  9  Error:  0.0 %\n"
     ]
    }
   ],
   "source": [
    "evaluate(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Discussion\"></a>\n",
    "## Discussion\n",
    "\n",
    "This seems a little fishy. This `evaluate` function shows that I can predict class labels with 0 error, meaning the decision tree is fully determined. It's unclear if this is the intent, or if I'm somehow skewing / overfitting my data. Given the propensity for over fitting, it is possible this is the natural outcome of this decision tree and dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Did you provide output exactly as requested?\n",
    "2. Did you re-execute the entire notebook? (\"Restart Kernel and Rull All Cells...\")\n",
    "3. If you did not complete the assignment or had difficulty please explain what gave you the most difficulty in the Markdown cell below.\n",
    "4. Did you change the name of the file to `jhed_id.ipynb`?\n",
    "\n",
    "Do not submit any other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit Notes\n",
    "\n",
    "* Initial submittal Sunday 3/17/24\n",
    "    * Submittal not complete - family health news resulting in delay\n",
    "    * Messaged professor via Canvas on 3/17/24; will resubmit complete assignment and notify professor when submitted\n",
    "    * Current progress stopped in the middle of debugging the `id3` algorithm\n",
    " \n",
    "* Resubmit Notes 3/24/24\n",
    "    * Error Rate: seems odd that I'd be showing 0% overall - perhaps that's just overfitting? \n",
    "    * Not sure how to handle `labeled=False`- I may have needed to pass this label into other functions like `id3` and `pick_best_attribute`.\n",
    "    * Overall I feel my code is far more complex than it needs to be... I did struggle a bit with understanding this algorithm and figuring out the recursion. I welcome any feedback you have on how I could have simplified this! \n",
    "    * Thank you again for the extension on this assignment - I truly appreciate your understanding! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
