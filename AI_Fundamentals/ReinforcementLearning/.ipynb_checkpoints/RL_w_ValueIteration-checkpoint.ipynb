{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 11 - Programming Assignment\n",
    "\n",
    "## Directions\n",
    "\n",
    "1. Change the name of this file to be your JHED id as in `jsmith299.ipynb`. Because sure you use your JHED ID (it's made out of your name and not your student id which is just letters and numbers).\n",
    "2. Make sure the notebook you submit is cleanly and fully executed. I do not grade unexecuted notebooks.\n",
    "3. Submit your notebook back in Blackboard where you downloaded this file.\n",
    "\n",
    "*Provide the output **exactly** as requested*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning with Value Iteration\n",
    "\n",
    "These are the same maps from Module 1 but the \"physics\" of the world have changed. In Module 1, the world was deterministic. When the agent moved \"south\", it went \"south\". When it moved \"east\", it went \"east\". Now, the agent only succeeds in going where it wants to go *sometimes*. There is a probability distribution over the possible states so that when the agent moves \"south\", there is a small probability that it will go \"east\", \"north\", or \"west\" instead and have to move from there.\n",
    "\n",
    "There are a variety of ways to handle this problem. For example, if using A\\* search, if the agent finds itself off the solution, you can simply calculate a new solution from where the agent ended up. Although this sounds like a really bad idea, it has actually been shown to work really well in video games that use formal planning algorithms (which we will cover later). When these algorithms were first designed, this was unthinkable. Thank you, Moore's Law!\n",
    "\n",
    "Another approach is to use Reinforcement Learning which covers problems where there is some kind of general uncertainty in the actions. We're going to model that uncertainty a bit unrealistically here but it'll show you how the algorithm works.\n",
    "\n",
    "As far as RL is concerned, there are a variety of options there: model-based and model-free, Value Iteration, Q-Learning and SARSA. You are going to use Value Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The World Representation\n",
    "\n",
    "As before, we're going to simplify the problem by working in a grid world. The symbols that form the grid have a special meaning as they specify the type of the terrain and the cost to enter a grid cell with that type of terrain:\n",
    "\n",
    "```\n",
    "token   terrain    cost \n",
    ".       plains     1\n",
    "*       forest     3\n",
    "^       hills      5\n",
    "~       swamp      7\n",
    "x       mountains  impassible\n",
    "```\n",
    "\n",
    "When you go from a plains node to a forest node it costs 3. When you go from a forest node to a plains node, it costs 1. You can think of the grid as a big graph. Each grid cell (terrain symbol) is a node and there are edges to the north, south, east and west (except at the edges).\n",
    "\n",
    "There are quite a few differences between A\\* Search and Reinforcement Learning but one of the most salient is that A\\* Search returns a plan of N steps that gets us from A to Z, for example, A->C->E->G.... Reinforcement Learning, on the other hand, returns  a *policy* that tells us the best thing to do in **every state.**\n",
    "\n",
    "For example, the policy might say that the best thing to do in A is go to C. However, we might find ourselves in D instead. But the policy covers this possibility, it might say, D->E. Trying this action might land us in C and the policy will say, C->E, etc. At least with offline learning, everything will be learned in advance (in online learning, you can only learn by doing and so you may act according to a known but suboptimal policy).\n",
    "\n",
    "Nevertheless, if you were asked for a \"best case\" plan from (0, 0) to (n-1, n-1), you could (and will) be able to read it off the policy because there is a best action for every state. You will be asked to provide this in your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the same costs as before. Note that we've negated them this time because RL requires negative costs and positive rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': -1, '*': -3, '^': -5, '~': -7}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs = { '.': -1, '*': -3, '^': -5, '~': -7}\n",
    "costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a list of offsets for `cardinal_moves`. You'll need to work this into your **actions**, A, parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardinal_moves = [(0,-1), (1,0), (0,1), (-1,0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Value Iteration, we require knowledge of the *transition* function, as a probability distribution.\n",
    "\n",
    "The transition function, T, for this problem is 0.70 for the desired direction, and 0.10 each for the other possible directions. That is, if the agent selects \"north\" then 70% of the time, it will go \"north\" but 10% of the time it will go \"east\", 10% of the time it will go \"west\", and 10% of the time it will go \"south\". If agent is at the edge of the map, it simply bounces back to the current state.\n",
    "\n",
    "You need to implement `value_iteration()` with the following parameters:\n",
    "\n",
    "+ world: a `List` of `List`s of terrain (this is S from S, A, T, gamma, R)\n",
    "+ costs: a `Dict` of costs by terrain (this is part of R)\n",
    "+ goal: A `Tuple` of (x, y) stating the goal state.\n",
    "+ reward: The reward for achieving the goal state.\n",
    "+ actions: a `List` of possible actions, A, as offsets.\n",
    "+ gamma: the discount rate\n",
    "\n",
    "you will return a policy: \n",
    "\n",
    "`{(x1, y1): action1, (x2, y2): action2, ...}`\n",
    "\n",
    "Remember...a policy is what to do in any state for all the states. Notice how this is different than A\\* search which only returns actions to take from the start to the goal. This also explains why reinforcement learning doesn't take a `start` state.\n",
    "\n",
    "You should also define a function `pretty_print_policy( cols, rows, policy)` that takes a policy and prints it out as a grid using \"^\" for up, \"<\" for left, \"v\" for down and \">\" for right. Use \"x\" for any mountain or other impassable square. Note that it doesn't need the `world` because the policy has a move for every state. However, you do need to know how big the grid is so you can pull the values out of the `Dict` that is returned.\n",
    "\n",
    "```\n",
    "vvvvvvv\n",
    "vvvvvvv\n",
    "vvvvvvv\n",
    ">>>>>>v\n",
    "^^^>>>v\n",
    "^^^>>>v\n",
    "^^^>>>G\n",
    "```\n",
    "\n",
    "(Note that that policy is completely made up and only illustrative of the desired output). Please print it out exactly as requested: **NO EXTRA SPACES OR LINES**.\n",
    "\n",
    "* If everything is otherwise the same, do you think that the path from (0,0) to the goal would be the same for both A\\* Search and Q-Learning?\n",
    "* What do you think if you have a map that looks like:\n",
    "\n",
    "```\n",
    "><>>^\n",
    ">>>>v\n",
    ">>>>v\n",
    ">>>>v\n",
    ">>>>G\n",
    "```\n",
    "\n",
    "has this converged? Is this a \"correct\" policy? What are the problems with this policy as it is?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_world(filename):\n",
    "    result = []\n",
    "    with open(filename) as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) > 0:\n",
    "                result.append(list(line.strip()))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Callable\n",
    "from copy import deepcopy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_world = read_world( \"small.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_cost\"></a>\n",
    "### get_cost\n",
    "\n",
    "`get_cost` takes in the world, current state, and cost dictionary to generate the cost of a state in the map. The state (cartesian position) is translated to the world map, and the corresponding symbol is checked in the dictionary. \n",
    "\n",
    "* **world**: List[List[str]]: Input world map\n",
    "* **current_state**: Tuple[int,int]: cartesian position of the current state on the map\n",
    "* **costs**: Dict[str,int]: dictionary of possible symbols and associated cost to occupy / move\n",
    "\n",
    "**returns** cost: Integer value of map cost for given location\n",
    "\n",
    "<div style=\"background: #4682b4\">\n",
    "Updated in resubmit to incorporate goal and reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New version, incorp goal\n",
    "def get_cost(world: List[List[str]], current_state: Tuple[int, int], costs: Dict[str,int], goal: Tuple[int, int], reward: int): \n",
    "    cur_x, cur_y = current_state[0], current_state[1]\n",
    "    symbol = world[current_state[0]][current_state[1]]\n",
    "    if symbol == 'x': \n",
    "        cost = -9999\n",
    "    elif cur_x == goal[0] and cur_y == goal[1]: #goal reached\n",
    "        cost = reward\n",
    "    else: \n",
    "        cost = costs[symbol]\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assertions / unit tests\n",
    "world = small_world\n",
    "goal = [4, 4]\n",
    "reward = 100\n",
    "\n",
    "current_state = [0,0]\n",
    "cost = get_cost(world, current_state, costs, goal, reward)\n",
    "assert cost == -1\n",
    "\n",
    "current_state = [0,1]\n",
    "cost = get_cost(world, current_state, costs, goal, reward)\n",
    "assert cost == -1\n",
    "\n",
    "current_state = [1,3]\n",
    "cost = get_cost(world, current_state, costs, goal, reward)\n",
    "assert cost == -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_direction\"></a>\n",
    "### get_direction\n",
    "\n",
    "`get_direction` takes in the current state, and next move (state), and outputs a direction in the form of a carrot direction.\n",
    "\n",
    "* **current**: Tuple[int,int]: current state (cartesian)\n",
    "* **next**: Tuple[int,int]: next state (cartesian)\n",
    "\n",
    "**returns** emoji: orientation of next move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_direction(current: Tuple[int, int], next: Tuple[int, int]): \n",
    "    x1, y1 = current[0], current[1]\n",
    "    x2, y2 = next[0], next[1]\n",
    "    dy = x2 - x1 #translated for this world\n",
    "    dx = y2 - y1 #translated for this world\n",
    "    if dx == 0 and dy > 0: #pos y move\n",
    "        return 'v'\n",
    "    if dx == 0 and dy < 0: #neg y move\n",
    "        return '^'\n",
    "    if dx > 0 and dy == 0: #pos x move\n",
    "        return '>'\n",
    "        # return '<' #debug\n",
    "    if dx < 0 and dy == 0: #neg x move\n",
    "        return '<'\n",
    "        # return '>' #debug\n",
    "    else: \n",
    "        return '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assertions / unit tests\n",
    "current = [0,0]\n",
    "next = [1,0]\n",
    "assert get_direction(current, next) == 'v'\n",
    "\n",
    "current = [0,0]\n",
    "next = [0,1]\n",
    "assert get_direction(current, next) == '>'\n",
    "\n",
    "current = [0,1]\n",
    "next = [0,0]\n",
    "assert get_direction(current, next) == '<'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current = [0,0]\n",
    "next = [1,1]\n",
    "get_direction(current, next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"is_valid_move\"></a>\n",
    "### is_valid_move\n",
    "\n",
    "`is_valid_move` checks a given position against the world, and returns a boolean for whether that move is valid and in-bounds. \n",
    "\n",
    "* **world**: List[List[str]]: current world map\n",
    "* **cur_pos**: (x, y) position on map\n",
    "\n",
    "**returns** Boolean, True if valid move, else False\n",
    "\n",
    "<div style=\"background: #4682b4\">\n",
    "New helper function for resubmit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_move(world, next_pos): \n",
    "    if (next_pos[0] < 0) or (next_pos[0] >= len(world)) or (next_pos[1] < 0) or (next_pos[1] >= len(world[0])): \n",
    "        return False #invalid move\n",
    "    else: \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIT TESTS\n",
    "assert is_valid_move(small_world, [0,-1]) == False\n",
    "assert is_valid_move(small_world, [0,0]) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_bestQ\"></a>\n",
    "### get_bestQ\n",
    "\n",
    "`get_bestQ` takes in the world, rewards, actions, costs, gamma, current position, and V_last matrix to determine the best Q value and direction of a move. Assertions are done in the value iteration testing. \n",
    "\n",
    "* **world**: List[List[str]]: current world map\n",
    "* **rewards**: List[float]: percent chances of each move (planned vs unplanned)\n",
    "* **actions**: List of possible moves\n",
    "* **costs**: Dictionary of costs for each state in the map\n",
    "* **gamma** Float discount factor\n",
    "* **cur_pos**: (x, y) position on map\n",
    "* **V_last**: previous V matrix\n",
    "\n",
    "**returns** Qbest (value) and direction (carrot)\n",
    "\n",
    "<div style=\"background: #4682b4\">\n",
    "Significantly modified for resubmission after feedback; iterating over actions rather than neighbors, and fixed errors in Q calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bestQ(world, reward, actions, costs, gamma, cur_pos, V_last, goal): \n",
    "    Qbest, direction = -1000, '?' #init\n",
    "    for action in actions:\n",
    "        next_pos = tuple(map(sum, zip(cur_pos, action)))\n",
    "        if not is_valid_move(world, next_pos): \n",
    "            continue\n",
    "        R = get_cost(world, next_pos, costs, goal, reward)\n",
    "        planned = 0.7 * V_last[next_pos[0]][next_pos[1]]\n",
    "        other_actions = deepcopy(actions)\n",
    "        other_actions.remove(action)\n",
    "        arg = 0\n",
    "        for oact in other_actions: \n",
    "            surprise_pos = tuple(map(sum, zip(cur_pos, oact))) \n",
    "            if not is_valid_move(world, surprise_pos): \n",
    "                continue\n",
    "            arg += 0.1 * V_last[surprise_pos[0]][surprise_pos[1]]\n",
    "        Q = R + gamma * (planned + arg)\n",
    "        if Q > Qbest: \n",
    "            direction, Qbest = get_direction(cur_pos, next_pos), Q\n",
    "    return Qbest, direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"max_V\"></a>\n",
    "### max_V\n",
    "\n",
    "`max_V` calculates the max absolute value between V and V_last, position by position.\n",
    "\n",
    "* **V**: Current value matrix\n",
    "* **V_last**: Value matrix from last state\n",
    "\n",
    "**returns** max absolute value difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_v(V, V_last): \n",
    "    assert len(V) == len(V_last)\n",
    "    assert len(V[0]) == len(V_last[0])\n",
    "    max_diff = 0\n",
    "    \n",
    "    for i in range(len(V)): \n",
    "        for j in range(len(V[0])): \n",
    "            diff = np.abs(V[i][j] - V_last[i][j])\n",
    "            if diff > max_diff: \n",
    "                max_diff = diff\n",
    "        \n",
    "    return max_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unit tests / assertions\n",
    "x = [[3, 3, 3], \n",
    "     [3, 3, 3], \n",
    "     [3, 3, 3]]\n",
    "y = [[1, 1, 1],\n",
    "     [1, 1, 1],\n",
    "     [1, 1, 1]]\n",
    "assert max_v(x, y) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"value_iteration\"></a>\n",
    "### value_iteration\n",
    "\n",
    "`value_iteration` is the main driving function to iterate through each step, calculate the best Q, and determine a policy for the world. \n",
    "\n",
    "* **world**: List[List[str]]: current world map\n",
    "* **goal** (x, y) goal state\n",
    "* **rewards**: List[float]: percent chances of each move (planned vs unplanned)\n",
    "* **actions**: List of possible moves\n",
    "* **costs**: Dictionary of costs for each state in the map\n",
    "* **gamma** Float discount factor\n",
    "\n",
    "**returns** policy dictionary for direction at each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(world, costs, goal, reward, actions, gamma):\n",
    "    V = [[0]*len(world[0]) for i in range(len(world))]    # init 0\n",
    "    V_last = [[-100]*len(world[0]) for i in range(len(world))]    # init -100 to show large error\n",
    "    eps, t = 0.01, 0 #init epsilon & t\n",
    "    policy = {}\n",
    "    \n",
    "    while (max_v(V, V_last) > eps) and (t < 100): \n",
    "        t+=1 \n",
    "        prev_max_v = max_v(V, V_last)\n",
    "        V_last = deepcopy(V)\n",
    "        for i in range(len(world)): \n",
    "            for j in range(len(world[0])): \n",
    "                cur_pos = (i, j)\n",
    "                Qbest, direction = get_bestQ(world, reward, actions, costs, gamma, cur_pos, V_last, goal) #function\n",
    "                policy[(i, j)] = direction \n",
    "                if world[i][j] not in costs: #move not allowed\n",
    "                    policy[(i,j)] = 'X'\n",
    "                V[i][j] = Qbest        \n",
    "    policy[goal] = 'G' #added in revision\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pretty_print_policy\"></a>\n",
    "### pretty_print_policy\n",
    "\n",
    "`pretty_print_policy` converts the policy dictionary into List form, then prints each row. \n",
    "\n",
    "* **cols**: number of columns\n",
    "* **rows**: number of rows\n",
    "* **policy**: dictionary policy for each state, providing a direction carrot\n",
    "* **goal**: final goal state\n",
    "\n",
    "**returns** None (print statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pretty_print_policy(cols, rows, policy, goal): \n",
    "    pol = [['?']*cols for i in range(rows)]\n",
    "    \n",
    "    for key in policy.keys(): \n",
    "        i, j = key[0], key[1]\n",
    "        pol[i][j] = policy[key]\n",
    "    \n",
    "    for row in pol: \n",
    "        print(*row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "### Small World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_world = read_world( \"small.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "goal = (len(small_world)-1, len(small_world[0])-1)\n",
    "gamma = 0.9\n",
    "reward = 10000\n",
    "\n",
    "small_policy = value_iteration(small_world, costs, goal, reward, cardinal_moves, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v v v v v v\n",
      "> v v v v v\n",
      "> > v > v v\n",
      "> > v X v v\n",
      "> > > v v v\n",
      "> > > > > v\n",
      "> > > > > G\n"
     ]
    }
   ],
   "source": [
    "cols = len(small_world[0])\n",
    "rows = len(small_world)\n",
    "\n",
    "pretty_print_policy(cols, rows, small_policy, goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_world = read_world( \"large.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "goal = (len(large_world[0])-1, len(large_world)-1)\n",
    "gamma = 0.9\n",
    "reward = 10000\n",
    "\n",
    "large_policy = value_iteration(large_world, costs, goal, reward, cardinal_moves, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v < < < < < v < > > > v v v > v v v < > > > > > > v v\n",
      "^ < ^ ^ ^ < < < v > > > > > v > v < X X X X X X X v <\n",
      "^ < < ^ X X ^ > v > > > > > > v v X X X v v < X X v v\n",
      "^ < < < < X X X v v v > > > > > v v v v v v < X X v v\n",
      "v < < < < X X v v v v > > > > > > > v v v v X X X v <\n",
      "v < ^ < X X v v v v > > > > > > > > > v v v v X v v <\n",
      "v < v X X > > v v v v ^ X X X > > > > > v v v v v v <\n",
      "v v v > > > v v v v v v < v X X X v v v > v v v v v <\n",
      "v > v > > > v v v < < < < < X X v v > v > > v v v v <\n",
      "v > ^ > > > > v v v < X X X X > > v v v > > > v v v <\n",
      "v < > > > v v v v < X X X > > > > > v v X X X v v v <\n",
      "^ > > > > v v v v X X v > > > > > > > v v X X v v v <\n",
      "v > > > > v v v v X X v v > > > > > > > v X v v v v <\n",
      "v v > > > > v v v v v v > > > > > > > > > > > v v v <\n",
      "> > > ^ X > > > v v v v > > > > > > > > > ^ X v v v <\n",
      "> v ^ X X X > > > v v v X X X > > > > > ^ X X v v v <\n",
      "> v X X > > > > > > > v v v X X X > ^ X X X v v v v <\n",
      "> v v X X > > > > > > > > > v v X X X X v v v v v v <\n",
      "> v v X X X > > > > > > > > > v v v v v v v v v v v <\n",
      "> v v v X X X > > > > > > > > > > > > > > > v v v v <\n",
      "> v v v v v X X > > > > > ^ X > > > > v > v v v v v v\n",
      "> > > v v v v X X X > ^ X X > > > > > > > > v v v v v\n",
      "> > > > > > > > v X X X X > > > > > > > > > > v v v v\n",
      "> > > > > > > > > > > > > > > > ^ X X > > > > > v v v\n",
      "^ X > > > > > > > > ^ X X X > ^ X X v X X > > > > v v\n",
      "^ X X X > > > > ^ ^ ^ < X X X X > > > v X X X > > v v\n",
      "> > > > ^ ^ ^ ^ ^ ^ ^ > > > > > > > > > > > > > > > G\n"
     ]
    }
   ],
   "source": [
    "cols = len(large_world[0])\n",
    "rows = len(large_world)\n",
    "\n",
    "pretty_print_policy(cols, rows, large_policy, goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Did you provide output exactly as requested?\n",
    "2. Did you re-execute the entire notebook? (\"Restart Kernel and Rull All Cells...\")\n",
    "3. If you did not complete the assignment or had difficulty please explain what gave you the most difficulty in the Markdown cell below.\n",
    "4. Did you change the name of the file to `jhed_id.ipynb`?\n",
    "\n",
    "Do not submit any other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Notes\n",
    "* I really struggled at the end here and ran out of time debugging\n",
    "* My policy seems all wonky, and I'm not sure how to really incorporate the goal state - I think that was a major oversight. I wasn't sure if we needed more of a heuristic function like in A*, or some other method.\n",
    "* I also have an unknown issue in my pretty print function that seems to be repeat printing one line of the policy; I added an extra print line to show my dictionary policy is outputting but there seems to be an issue with the print function. \n",
    "* I got confused on rewards vs probabilities, and I think my `reward` and `R` usage is not correct\n",
    "* As always - I'm interested in any feedback you can provide for how I should improve this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Submit Feedback\n",
    "- You don't need the heuristic function.\n",
    "- The two things that matters are:\n",
    "    - The cost of moving from a state to the next one\n",
    "    - The probability of ending up in the next state times the value of that next state\n",
    "- rewards is a bit of a mis-name. It should be probabilities of move\n",
    "- The heuristic R value is what's messing things up here.\n",
    "- You don't need that. At each step, the only thing the agent cares about is trying to find the next location that has the best value.\n",
    "- I think you're really really close. It's just that your math is a little off:\n",
    "    - (planned + sum(surprise)) shouldn't both be multiplied times V_last\n",
    "- Let me know if you don't get it quickly. Happy to help pair for 10 minutes.\n",
    "- 5/10 (Revise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback in 1:1 Meeting\n",
    "- Iterate over actions rather than neighbors\n",
    "- Fix Q calculation and how you calculate next state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes Made in Resubmit\n",
    "- `pretty_print_policy` rewritten / debugged - was previously not printing the policy, and instead only printed the same row on repeat\n",
    "- `get_bestQ` updated to fix calculation errors in Q value; adjusted to iterate over actions rather than neighbors\n",
    "- `get_cost`updated to incorporate reward and goal state check\n",
    "- `heuristic` removed\n",
    "- `get_neighbors` removed\n",
    "- `is_valid_move` added\n",
    "- `reward` removed (was originally showing probabilities of moves - those are now hard-coded in `get_bestQ`.\n",
    "- `value_iteration` updated to fix loop errors where only one iteration was running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Removed for Resubmission: No longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_neighbors\"></a>\n",
    "### get_neighbors\n",
    "\n",
    "`get_neighbors` takes in the world, current state, and possible moves to generate a list of all valid children of the current state. Validity is checked against the bounds of the given world. \n",
    "\n",
    "* **world**: List[List[str]]: Input world map\n",
    "* **current_state**: Tuple[int,int]: cartesian position of the current state on the map\n",
    "* **moves**: List[(int, int)]: list of possible valid moves in cartesian space\n",
    "* **costs**: Dict[str,int]: dictionary of possible symbols and associated cost to occupy / move\n",
    "\n",
    "**returns** neighbors: List of valid neighbors (children) of current state in List[Tuple[int,int]] form\n",
    "\n",
    "<div style=\"background: #4682b4\">\n",
    "Commented out for resubmit - no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_neighbors(world: List[List[str]], current_state: Tuple[int, int], moves, \n",
    "#                   costs: Dict[str, int]): \n",
    "#     neighbors = []\n",
    "#     for move in moves: \n",
    "#         x_move, y_move = move[0], move[1]\n",
    "#         # print('current_state: ', current_state, ' x_move: ', x_move)\n",
    "#         test_x, test_y = current_state[0]+x_move, current_state[1]+y_move\n",
    "#         # print('current state: ', current_state, 'testx, testy', test_x, test_y)\n",
    "        \n",
    "#         if (test_x < 0) or (test_y < 0): #out of bounds - negative\n",
    "#             continue\n",
    "#         if (test_x >= len(world)) or (test_y >= len(world[0])): #out of bounds\n",
    "#             continue\n",
    "#         if world[test_x][test_y] not in costs: #move not allowed\n",
    "#             continue\n",
    "\n",
    "#         neighbor = [test_x, test_y]\n",
    "#         neighbors.append(neighbor)\n",
    "    \n",
    "#     return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #assertions / unit tests\n",
    "# world = small_world #7 rows x 6 columns\n",
    "# moves = cardinal_moves\n",
    "# current_state = [0,0]\n",
    "# neighbors = get_neighbors(world, current_state, moves, costs)\n",
    "# assert neighbors == [[1,0], [0,1]]\n",
    "\n",
    "# current_state = [3,3]\n",
    "# neighbors = get_neighbors(world, current_state, moves, costs)\n",
    "# assert neighbors == [[3,2], [4,3], [3,4], [2,3]]\n",
    "\n",
    "# current_state = [6, 5]\n",
    "# neighbors = get_neighbors(world, current_state, moves, costs)\n",
    "# assert neighbors == [[6,4], [5,5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"heuristic\"></a>\n",
    "### heuristic\n",
    "\n",
    "`heuristic` takes in the current state and goal state, and calculates the euclidean distance between the two points. **Used by**: [get_next_state](#get_next_state)\n",
    "\n",
    "* **state** Tuple[int,int]: cartesian position of the current state on the map\n",
    "* **goal**: Tuple[int,int]: cartesian position of the goal state on the map\n",
    "\n",
    "**returns** euc: Float euclidean distance between state and goal\n",
    "\n",
    "<div style=\"background: #4682b4\">\n",
    "Commented out for Resubmit - no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# def heuristic(state: Tuple[int,int], goal: Tuple[int,int]):\n",
    "#     euc = np.sqrt((state[0] - goal[0])**2 + (state[1] - goal[1])**2)\n",
    "\n",
    "#     return euc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assertions/unit tests\n",
    "# state = [1,1]\n",
    "# goal = [2,1]\n",
    "# euc = heuristic(state, goal)\n",
    "# assert euc == 1\n",
    "\n",
    "# state = [0,0]\n",
    "# goal = [5,5]\n",
    "# euc = heuristic(state, goal)\n",
    "# assert euc == np.sqrt(50)\n",
    "\n",
    "# state = [5, 6]\n",
    "# goal = [1,2]\n",
    "# euc = heuristic(state, goal)\n",
    "# assert euc == np.sqrt(32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "171px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
